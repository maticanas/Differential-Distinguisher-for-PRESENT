{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXVzTjGSQpAx",
    "outputId": "31cbc208-81e9-4251-95e5-a53c46575581"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 80-bit Key Vectors:\n",
      "Success 0x5579c1387b228445\n",
      "Success 0xe72c46c0f5945049\n",
      "Success 0xa112ffc72f68417b\n",
      "Success 0x3333dcd3213210d2\n",
      "0x5579c1387b228445\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://github.com/inmcm/present_cipher/tree/master/python\n",
    "\"\"\"\n",
    "# coding: utf-8\n",
    "from __future__ import print_function\n",
    "\n",
    "s_box = (0xC, 0x5, 0x6, 0xB, 0x9, 0x0, 0xA, 0xD, 0x3, 0xE, 0xF, 0x8, 0x4, 0x7, 0x1, 0x2)\n",
    "\n",
    "inv_s_box = (0x5, 0xE, 0xF, 0x8, 0xC, 0x1, 0x2, 0xD, 0xB, 0x4, 0x6, 0x3, 0x0, 0x7, 0x9, 0xA)\n",
    "\n",
    "p_layer_order = [0, 16, 32, 48, 1, 17, 33, 49, 2, 18, 34, 50, 3, 19, 35, 51, 4, 20, 36, 52, 5, 21, 37, 53, 6, 22, 38,\n",
    "                 54, 7, 23, 39, 55, 8, 24, 40, 56, 9, 25, 41, 57, 10, 26, 42, 58, 11, 27, 43, 59, 12, 28, 44, 60, 13,\n",
    "                 29, 45, 61, 14, 30, 46, 62, 15, 31, 47, 63]\n",
    "\n",
    "block_size = 64\n",
    "\n",
    "ROUND_LIMIT = 32\n",
    "\n",
    "\n",
    "def round_function(state, key):\n",
    "    new_state = state ^ key\n",
    "    state_nibs = []\n",
    "    for x in range(0, block_size, 4):\n",
    "        nib = (new_state >> x) & 0xF\n",
    "        sb_nib = s_box[nib]\n",
    "        state_nibs.append(sb_nib)\n",
    "    # print(state_nibs)\n",
    "\n",
    "    state_bits = []\n",
    "    for y in state_nibs:\n",
    "        nib_bits = [1 if t == '1'else 0 for t in format(y, '04b')[::-1]]\n",
    "        state_bits += nib_bits\n",
    "    # print(state_bits)\n",
    "    # print(len(state_bits))\n",
    "\n",
    "    state_p_layer = [0 for _ in range(64)]\n",
    "    for p_index, std_bits in enumerate(state_bits):\n",
    "        state_p_layer[p_layer_order[p_index]] = std_bits\n",
    "\n",
    "    # print(len(state_p_layer), state_p_layer)\n",
    "\n",
    "    round_output = 0\n",
    "    for index, ind_bit in enumerate(state_p_layer):\n",
    "        round_output += (ind_bit << index)\n",
    "\n",
    "    # print(format(round_output, '#016X'))\n",
    "\n",
    "    # print('')\n",
    "    return round_output\n",
    "\n",
    "\n",
    "def key_function_80(key, round_count):\n",
    "    # print('Start: ', hex(key))\n",
    "    # print('')\n",
    "\n",
    "    r = [1 if t == '1'else 0 for t in format(key, '080b')[::-1]]\n",
    "\n",
    "    # print('k bits:', r)\n",
    "    # print('')\n",
    "\n",
    "    h = r[-61:] + r[:-61]\n",
    "\n",
    "    # print('s bits:', h)\n",
    "    # print('')\n",
    "\n",
    "    round_key_int = 0\n",
    "    # print('init round int:', hex(round_key_int))\n",
    "    for index, ind_bit in enumerate(h):\n",
    "        round_key_int += (ind_bit << index)\n",
    "        # print('round:',index, '-', hex(round_key_int))\n",
    "\n",
    "    # print('round_key_int', hex(round_key_int))\n",
    "    # print('')\n",
    "\n",
    "    upper_nibble = round_key_int >> 76\n",
    "\n",
    "    # print('upper_nibble:', upper_nibble)\n",
    "\n",
    "    upper_nibble = s_box[upper_nibble]\n",
    "\n",
    "    # print('upper_nibble sboxed', hex(upper_nibble))\n",
    "\n",
    "    xor_portion = ((round_key_int >> 15) & 0x1F) ^ round_count\n",
    "    # print('Count:', round_count)\n",
    "    # print('XOR Value:', xor_portion)\n",
    "\n",
    "    # print('Before:', hex(round_key_int))\n",
    "    round_key_int = (round_key_int & 0x0FFFFFFFFFFFFFF07FFF) + (upper_nibble << 76) + (xor_portion << 15)\n",
    "    # print('After: ', hex(round_key_int))\n",
    "\n",
    "    return round_key_int\n",
    "\n",
    "\n",
    "\n",
    "test_vectors_80 = {1:(0x00000000000000000000, 0x0000000000000000, 0x5579C1387B228445),\n",
    "                2:(0xFFFFFFFFFFFFFFFFFFFF, 0x0000000000000000, 0xE72C46C0F5945049),\n",
    "                3:(0x00000000000000000000, 0xFFFFFFFFFFFFFFFF, 0xA112FFC72F68417B),\n",
    "                4:(0xFFFFFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF, 0x3333DCD3213210D2)}\n",
    "\n",
    "print('Testing 80-bit Key Vectors:')\n",
    "\n",
    "\n",
    "\n",
    "for test_case in test_vectors_80:\n",
    "\n",
    "    key_schedule = []\n",
    "    current_round_key = test_vectors_80[test_case][0]\n",
    "    round_state = test_vectors_80[test_case][1]\n",
    "\n",
    "    # Key schedule\n",
    "    for rnd_cnt in range(ROUND_LIMIT):\n",
    "        # print(format(round_key, '020X'))\n",
    "        # print(format(round_key >> 16, '016X'))\n",
    "        key_schedule.append(current_round_key >> 16)\n",
    "        current_round_key = key_function_80(current_round_key, rnd_cnt + 1)\n",
    "\n",
    "    for rnd in range(ROUND_LIMIT - 1):\n",
    "        # print('Round:', rnd)\n",
    "        # print('State:', format(round_state, '016X'))\n",
    "        # print('R_Key:', format(key_schedule[rnd], '016X'))\n",
    "        round_state = round_function(round_state, key_schedule[rnd])\n",
    "\n",
    "    round_state ^= key_schedule[ROUND_LIMIT-1]\n",
    "\n",
    "    if round_state == test_vectors_80[test_case][2]:\n",
    "        print('Success', hex(round_state))\n",
    "    else:\n",
    "        print('Failure', hex(round_state))\n",
    "        \n",
    "def PRESENT(P, K, ROUND):\n",
    "    key_schedule = []\n",
    "    current_round_key = K\n",
    "    round_state = P\n",
    "    \n",
    "    if(ROUND==0):\n",
    "        return P\n",
    "\n",
    "    for rnd_cnt in range(ROUND):\n",
    "        key_schedule.append(current_round_key >> 16)\n",
    "        current_round_key = key_function_80(current_round_key, rnd_cnt + 1)\n",
    "\n",
    "    for rnd in range(ROUND - 1):\n",
    "        round_state = round_function(round_state, key_schedule[rnd])\n",
    "\n",
    "    round_state ^= key_schedule[ROUND-1]\n",
    "    \n",
    "    return round_state\n",
    "\n",
    "C = PRESENT(0x0, 0x0, ROUND=32)\n",
    "print(hex(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AwZVgDHVQpAz"
   },
   "outputs": [],
   "source": [
    "Wang_diff = [0x7000000000007000, 0x0700000000000700, 0x0070000000000070, 0x0007000000000007]\n",
    "BLOCK_SIZE = 64\n",
    "\n",
    "ROUND_global = 6\n",
    "sample_num = 10000\n",
    "test_sample_num = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xJrNRizYQpA0"
   },
   "outputs": [],
   "source": [
    "def gen(sample_num, ROUND):\n",
    "    P_set = []\n",
    "    K_set = []\n",
    "    for i in range(sample_num):\n",
    "        P_set.append(random.randrange(0,2**64))\n",
    "        #print(\"%x\" % P_set[i])\n",
    "        K_set.append(random.randrange(0,2**80))\n",
    "        #print(\"%x\" % K_set[i])\n",
    "\n",
    "    C_diff_set = []\n",
    "    C_diff_label = []\n",
    "    for i in range(sample_num):\n",
    "        P = P_set[i]\n",
    "        K = K_set[i]\n",
    "        C = PRESENT(P, K, ROUND)\n",
    "        for j in range(4):\n",
    "            Cj = PRESENT(P^Wang_diff[j], K, ROUND)\n",
    "            C_diff = C^Cj\n",
    "            #print(C_diff)\n",
    "            C_diff_set.append(C_diff)\n",
    "            temp = [0, 0, 0, 0]\n",
    "            temp[j] = 1\n",
    "            C_diff_label.append(temp)\n",
    "\n",
    "    tr_X = []\n",
    "    for C_diff in C_diff_set:\n",
    "        A = []\n",
    "        for j in range(BLOCK_SIZE):\n",
    "            A.append((C_diff>>j)&1)\n",
    "        tr_X.append(A)\n",
    "        #print(A)\n",
    "    tr_X = np.array(tr_X)\n",
    "\n",
    "    tr_X = []\n",
    "    for C_diff in C_diff_set:\n",
    "        A = []\n",
    "        for j in range(BLOCK_SIZE):\n",
    "            A.append((C_diff>>j)&1)\n",
    "        tr_X.append(A)\n",
    "        #print(A)\n",
    "    tr_X = np.array(tr_X)\n",
    "    tr_t = np.array(C_diff_label)\n",
    "\n",
    "    ind = np.arange(len(tr_X))\n",
    "    np.random.shuffle(ind)\n",
    "    tr_X = tr_X[ind]\n",
    "    tr_t = tr_t[ind]\n",
    "    \n",
    "    return tr_X, tr_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gen():\n",
    "    SAMPLE_NUM_RANGE = [10000, 50000, 100000]\n",
    "    ROUND_RANGE = [3, 4, 5, 6, 7, 8]\n",
    "    for sn in SAMPLE_NUM_RANGE:\n",
    "        for rn in ROUND_RANGE:\n",
    "            tr_X, tr_t = gen(sn, rn)\n",
    "            np.save(\"ROUND %d SAMPLE %d Dataset\" % (rn, sn), tr_X)\n",
    "            np.save(\"ROUND %d SAMPLE %d Label\" % (rn, sn), tr_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Wk0Lr4ReQpA1"
   },
   "outputs": [],
   "source": [
    "def test_sample_gen():\n",
    "    TEST_SMAPLE_NUM = 10000\n",
    "    for rn in ROUND_RANGE:\n",
    "        te_X, te_t = gen(TEST_SMAPLE_NUM, rn)\n",
    "        np.save(\"ROUND %d TEST_SAMPLE Dataset\" % (rn), te_X)\n",
    "        np.save(\"ROUND %d TEST_SAMPLE Label\" % (rn), te_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O9XPTV7FQpA1"
   },
   "outputs": [],
   "source": [
    "def load_sample(SAMPLE_NUM, ROUND_NUM):\n",
    "    tr_X = np.load(\"ROUND %d SAMPLE %d Dataset.npy\" % (ROUND_NUM, SAMPLE_NUM))\n",
    "    tr_t = np.load(\"ROUND %d SAMPLE %d Label.npy\" % (ROUND_NUM, SAMPLE_NUM))\n",
    "    return tr_X, tr_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_sample(ROUND_NUM):\n",
    "    te_X = np.load(\"ROUND %d TEST_SAMPLE Dataset.npy\" % (ROUND_NUM))\n",
    "    te_t = np.load(\"ROUND %d TEST_SAMPLE Label.npy\" % (ROUND_NUM))\n",
    "    return te_X, te_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RfujwI1AQpA1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer1=128, layer2=1028, layer3=None, reg=None, learning_rate=0.001):\n",
    "        self.layers = self._build_layers(layer1, layer2, layer3, reg)\n",
    "        self.model = tf.keras.Sequential(self.layers) \n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def _build_layers(self, layer1, layer2, layer3, reg):\n",
    "        if layer3==None:\n",
    "            layers = [\n",
    "                tf.keras.layers.Flatten(input_shape=(64,)),\n",
    "                tf.keras.layers.Dense(layer1, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer2, activation='relu', kernel_regularizer=reg),\n",
    "                #tf.keras.layers.Dense(layer3, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(4, activation='softmax')\n",
    "            ]\n",
    "        else:\n",
    "            layers = [\n",
    "                tf.keras.layers.Flatten(input_shape=(64,)),\n",
    "                tf.keras.layers.Dense(layer1, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer2, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer3, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(4, activation='softmax')\n",
    "            ]\n",
    "        return layers\n",
    "\n",
    "    #그냥 cross entropy를 그대로 정의함\n",
    "    def _my_loss(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32) #float32 => int32로 casting #None, 1\n",
    "        y_true = tf.squeeze(tf.one_hot(y_true, depth=10, dtype=tf.float32), 1) # one_hot encoding #None, 1, 10 #quezze => 1을 없애줌 => None, 10\n",
    "        y_pred = tf.nn.softmax(y_pred, 1) # 한 축에 대해 softmax를 적용해라 #1 => 열을 의미 #즉, 한 행에 있는 값을 다 더하면 1이 되도록 만들어줌\n",
    "\n",
    "        #cross entropy 그대로 적용\n",
    "        #-sum t*log y 한 후에 평균 냄\n",
    "        return -tf.reduce_mean(tf.reduce_sum(\n",
    "                tf.multiply(y_true, tf.math.log(y_pred)), 1))\n",
    "\n",
    "    def _my_accuracy(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        y_true = tf.squeeze(tf.one_hot(y_true, depth=10, dtype=tf.float32), 1)\n",
    "        #argmax를 그대로 이용\n",
    "        return tf.reduce_mean(\n",
    "            tf.cast(\n",
    "                tf.equal(tf.argmax(y_true, 1), tf.argmax(y_pred, 1)), tf.float32))\n",
    "\n",
    "    def fit(self, x, t, epochs, batch_size=None, validation_split=0.0, verbose=1, shuffle=False, workers=2):\n",
    "        self.model.fit(x, t, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=verbose, shuffle=shuffle, workers=workers)\n",
    "    \n",
    "    def evaluate(self, x=None, y=None, verbose=1):\n",
    "        return self.model.evaluate(x=x, y=y, verbose=verbose)\n",
    "    \n",
    "    def summary(self):\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhQTZ7KwQpA2",
    "outputId": "85b3d255-445f-463d-ba53-122475dfd771"
   },
   "outputs": [],
   "source": [
    "#Config\n",
    "ROUND = 6\n",
    "SAMPLE_NUM = 10000\n",
    "test_sample_num = 10000\n",
    "ITERATION = 5\n",
    "\n",
    "#Fix\n",
    "batch_size = 200\n",
    "epoch_size = 25\n",
    "validation_split = 0.3\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w53eKengQpA2"
   },
   "outputs": [],
   "source": [
    "def learn(ROUND, SAMPLE_NUM, layer1, layer2, layer3=None, reg=None, learning_rate=0.001):\n",
    "    tr_X, tr_t = load_sample(SAMPLE_NUM=SAMPLE_NUM, ROUND_NUM=ROUND)\n",
    "    te_X, te_t = load_test_sample(ROUND_NUM=ROUND)\n",
    "    accuracy = []\n",
    "    for i in range(ITERATION):\n",
    "        model = MLP(layer1, layer2, layer3, reg, learning_rate)\n",
    "        model.fit(tr_X, tr_t, epochs=epoch_size, batch_size=batch_size, validation_split = validation_split, shuffle=True, verbose=0)\n",
    "        accuracy.append(model.evaluate(te_X, te_t, verbose=0)[1])\n",
    "    avg = np.mean(np.array(accuracy))\n",
    "    #print(\"average : %f\" % avg)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_reg_L2(ROUND=ROUND, l1=256, l2=2048, weight=[0.0001]):\n",
    "    tr_X, tr_t = gen(sample_num, ROUND)\n",
    "    te_X, te_t = gen(test_sample_num, ROUND)\n",
    "\n",
    "    result = []\n",
    "    result_weight = []\n",
    "    for w in weight:\n",
    "        accuracy = []\n",
    "        for i in range(ITERATION):\n",
    "            model = MLP(layer1=l1, layer2=l2, reg=tf.keras.regularizers.L2(w))\n",
    "            model.fit(tr_X, tr_t, epochs=epoch_size, batch_size=batch_size, validation_split = validation_split, verbose=0)\n",
    "            accuracy.append(model.evaluate(te_X, te_t)[1])\n",
    "        avg_acc = np.mean(np.array(accuracy))\n",
    "        result.append(avg_acc)\n",
    "        result_weight.append(w)\n",
    "    return result, result_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 6s 5ms/step - loss: 2.7795 - accuracy: 0.2592\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 2.6142 - accuracy: 0.2587\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 2.8273 - accuracy: 0.2566\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.6379 - accuracy: 0.2594\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 2.9858 - accuracy: 0.2523\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4321 - accuracy: 0.2641\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4319 - accuracy: 0.2618\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4299 - accuracy: 0.2609\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.4287 - accuracy: 0.2611\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4446 - accuracy: 0.2596\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3863 - accuracy: 0.2500: 0s - loss: 1\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3863 - accuracy: 0.2500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e73c591a623c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearn_reg_L2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmaxidx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-f65c717d589c>\u001b[0m in \u001b[0;36mlearn_reg_L2\u001b[1;34m(ROUND, l1, l2, weight)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mITERATION\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mte_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mavg_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-e84bc6f24b43>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, t, epochs, batch_size, validation_split, verbose, shuffle, workers)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weight = [0.0001, 0.001, 0.01, 0.1]\n",
    "accuracy, accuracy_weight = learn_reg_L2(weight=weight)\n",
    "maxidx = accuracy.index(max(accuracy))\n",
    "print(accuracy)\n",
    "print(accuracy_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 6s 5ms/step - loss: 3.2773 - accuracy: 0.2575\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.8776 - accuracy: 0.2551\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.8714 - accuracy: 0.2537\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 3.1266 - accuracy: 0.2540\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 3.0272 - accuracy: 0.2562\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.4281 - accuracy: 0.2572\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4208 - accuracy: 0.2590\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.4309 - accuracy: 0.2575\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.4331 - accuracy: 0.2570\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.4276 - accuracy: 0.2580\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3902 - accuracy: 0.2601\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3895 - accuracy: 0.2606\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3892 - accuracy: 0.2594\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3888 - accuracy: 0.2581\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3898 - accuracy: 0.2604\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3873 - accuracy: 0.2569\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3873 - accuracy: 0.2565\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3879 - accuracy: 0.2584\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3875 - accuracy: 0.2573\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3875 - accuracy: 0.2576\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 7ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500: 0s - l\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3863 - accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "weight = [0, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009]\n",
    "accuracy, accuracy_weight = learn_reg_L2(weight=weight)\n",
    "maxidx = accuracy.index(max(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001, 0.0011, 0.0012000000000000001, 0.0013, 0.0014, 0.0015, 0.0015999999999999999, 0.0017000000000000001, 0.0018, 0.0019, 0.002, 0.0021000000000000003, 0.0021999999999999997, 0.0023, 0.0024000000000000002, 0.0025, 0.0026, 0.0027, 0.0028, 0.0029]\n"
     ]
    }
   ],
   "source": [
    "RANGE1 = 20\n",
    "weight2 = []\n",
    "if(maxidx==0):\n",
    "    base = 0\n",
    "else:\n",
    "    base = (maxidx-1)*0.001\n",
    "for i in range(RANGE1):\n",
    "    weight2.append(base + (i/10000.0))\n",
    "print(weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MVTYS1K6uVn2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4556 - accuracy: 0.2578\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4450 - accuracy: 0.2645\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4367 - accuracy: 0.2560\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4373 - accuracy: 0.2571\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4505 - accuracy: 0.2611\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.4372 - accuracy: 0.2603\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.4441 - accuracy: 0.2591\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4476 - accuracy: 0.2563\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4227 - accuracy: 0.2568\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4419 - accuracy: 0.2599\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4209 - accuracy: 0.2609\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4155 - accuracy: 0.2629\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4216 - accuracy: 0.2578\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4328 - accuracy: 0.2602\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4201 - accuracy: 0.2587\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.4248 - accuracy: 0.2585\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 1.4161 - accuracy: 0.2578\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 1.4185 - accuracy: 0.2599\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4133 - accuracy: 0.2623\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.4112 - accuracy: 0.2584\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.4131 - accuracy: 0.2584\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.4105 - accuracy: 0.2596\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4042 - accuracy: 0.2598\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4074 - accuracy: 0.2567\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4089 - accuracy: 0.2606\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4130 - accuracy: 0.2553\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4036 - accuracy: 0.2587\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.4072 - accuracy: 0.2570\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.4068 - accuracy: 0.2607\n",
      "1250/1250 [==============================] - 8s 7ms/step - loss: 1.4035 - accuracy: 0.2592\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.4000 - accuracy: 0.2567\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.4103 - accuracy: 0.2580\n",
      "1250/1250 [==============================] - 11s 8ms/step - loss: 1.4023 - accuracy: 0.2562\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 1.4006 - accuracy: 0.2585\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.4014 - accuracy: 0.2569\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 1.3988 - accuracy: 0.2566\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.3955 - accuracy: 0.2565\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4016 - accuracy: 0.2569\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3976 - accuracy: 0.2564\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4028 - accuracy: 0.2569\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3936 - accuracy: 0.2561\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3971 - accuracy: 0.2562\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3958 - accuracy: 0.2551\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3989 - accuracy: 0.2551\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3964 - accuracy: 0.2554\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3942 - accuracy: 0.2559\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3932 - accuracy: 0.2584\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3943 - accuracy: 0.2572\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3974 - accuracy: 0.2539\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3954 - accuracy: 0.2524\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3924 - accuracy: 0.2532\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3966 - accuracy: 0.2521\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3919 - accuracy: 0.2560\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3950 - accuracy: 0.2568\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3923 - accuracy: 0.2573\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3902 - accuracy: 0.2541\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3926 - accuracy: 0.2559\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3909 - accuracy: 0.2564\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3895 - accuracy: 0.2541\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3957 - accuracy: 0.2562\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3919 - accuracy: 0.2547\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3896 - accuracy: 0.2553\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3926 - accuracy: 0.2528\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3927 - accuracy: 0.2546\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3923 - accuracy: 0.2520\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3919 - accuracy: 0.2518\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3909 - accuracy: 0.2557\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3910 - accuracy: 0.2523\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3913 - accuracy: 0.2543\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3897 - accuracy: 0.2531\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3908 - accuracy: 0.2542\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3891 - accuracy: 0.2553\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3922 - accuracy: 0.2546\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3903 - accuracy: 0.2519\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3895 - accuracy: 0.2539\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3886 - accuracy: 0.2553\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3896 - accuracy: 0.2553\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3885 - accuracy: 0.2560\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3932 - accuracy: 0.2539\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3894 - accuracy: 0.2558\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3892 - accuracy: 0.2539\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3909 - accuracy: 0.2551\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3890 - accuracy: 0.2546\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3897 - accuracy: 0.2547\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3896 - accuracy: 0.2545\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3890 - accuracy: 0.2556\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3894 - accuracy: 0.2548\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3883 - accuracy: 0.2553\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3894 - accuracy: 0.2552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3890 - accuracy: 0.2546\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3928 - accuracy: 0.2528\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3884 - accuracy: 0.2553\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3886 - accuracy: 0.2549\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3895 - accuracy: 0.2543\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3888 - accuracy: 0.2539\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3889 - accuracy: 0.2562\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3892 - accuracy: 0.2528\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3887 - accuracy: 0.2548\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3893 - accuracy: 0.2548\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3892 - accuracy: 0.2560\n"
     ]
    }
   ],
   "source": [
    "accuracy, accuracy_weight = learn_reg_L2(weight=weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max weight idx : 2\n",
      "max weight : 0.001200\n",
      "max accuracy : 0.260095\n"
     ]
    }
   ],
   "source": [
    "maxidx1 = accuracy.index(max(accuracy))\n",
    "print(\"max weight idx : %d\" % maxidx1)\n",
    "print(\"max weight : %f\" % accuracy_weight[maxidx1])\n",
    "print(\"max accuracy : %f\" % accuracy[maxidx1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x200143126a0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(RANGE1)], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = 1/10000.0\n",
    "start = accuracy_weight[maxidx1] + (-1)*unit\n",
    "end = accuracy_weight[maxidx1] + (+1)*unit\n",
    "step_num = 100\n",
    "step = unit/step_num\n",
    "weight2 = np.arange(start, end, step)\n",
    "print(accuracy_weight[maxidx1])\n",
    "print(weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, accuracy_weight = learn_reg_L2(weight=weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxidx1 = accuracy.index(max(accuracy))\n",
    "print(\"max weight idx : %d\" % maxidx1)\n",
    "print(\"max weight : %f\" % accuracy_weight[maxidx1])\n",
    "print(\"max accuracy : %f\" % accuracy[maxidx1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(step_num*2+1)], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
