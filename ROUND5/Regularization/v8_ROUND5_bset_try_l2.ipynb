{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXVzTjGSQpAx",
    "outputId": "31cbc208-81e9-4251-95e5-a53c46575581"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 80-bit Key Vectors:\n",
      "Success 0x5579c1387b228445\n",
      "Success 0xe72c46c0f5945049\n",
      "Success 0xa112ffc72f68417b\n",
      "Success 0x3333dcd3213210d2\n",
      "0x5579c1387b228445\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://github.com/inmcm/present_cipher/tree/master/python\n",
    "\"\"\"\n",
    "# coding: utf-8\n",
    "from __future__ import print_function\n",
    "\n",
    "s_box = (0xC, 0x5, 0x6, 0xB, 0x9, 0x0, 0xA, 0xD, 0x3, 0xE, 0xF, 0x8, 0x4, 0x7, 0x1, 0x2)\n",
    "\n",
    "inv_s_box = (0x5, 0xE, 0xF, 0x8, 0xC, 0x1, 0x2, 0xD, 0xB, 0x4, 0x6, 0x3, 0x0, 0x7, 0x9, 0xA)\n",
    "\n",
    "p_layer_order = [0, 16, 32, 48, 1, 17, 33, 49, 2, 18, 34, 50, 3, 19, 35, 51, 4, 20, 36, 52, 5, 21, 37, 53, 6, 22, 38,\n",
    "                 54, 7, 23, 39, 55, 8, 24, 40, 56, 9, 25, 41, 57, 10, 26, 42, 58, 11, 27, 43, 59, 12, 28, 44, 60, 13,\n",
    "                 29, 45, 61, 14, 30, 46, 62, 15, 31, 47, 63]\n",
    "\n",
    "block_size = 64\n",
    "\n",
    "ROUND_LIMIT = 32\n",
    "\n",
    "\n",
    "def round_function(state, key):\n",
    "    new_state = state ^ key\n",
    "    state_nibs = []\n",
    "    for x in range(0, block_size, 4):\n",
    "        nib = (new_state >> x) & 0xF\n",
    "        sb_nib = s_box[nib]\n",
    "        state_nibs.append(sb_nib)\n",
    "    # print(state_nibs)\n",
    "\n",
    "    state_bits = []\n",
    "    for y in state_nibs:\n",
    "        nib_bits = [1 if t == '1'else 0 for t in format(y, '04b')[::-1]]\n",
    "        state_bits += nib_bits\n",
    "    # print(state_bits)\n",
    "    # print(len(state_bits))\n",
    "\n",
    "    state_p_layer = [0 for _ in range(64)]\n",
    "    for p_index, std_bits in enumerate(state_bits):\n",
    "        state_p_layer[p_layer_order[p_index]] = std_bits\n",
    "\n",
    "    # print(len(state_p_layer), state_p_layer)\n",
    "\n",
    "    round_output = 0\n",
    "    for index, ind_bit in enumerate(state_p_layer):\n",
    "        round_output += (ind_bit << index)\n",
    "\n",
    "    # print(format(round_output, '#016X'))\n",
    "\n",
    "    # print('')\n",
    "    return round_output\n",
    "\n",
    "\n",
    "def key_function_80(key, round_count):\n",
    "    # print('Start: ', hex(key))\n",
    "    # print('')\n",
    "\n",
    "    r = [1 if t == '1'else 0 for t in format(key, '080b')[::-1]]\n",
    "\n",
    "    # print('k bits:', r)\n",
    "    # print('')\n",
    "\n",
    "    h = r[-61:] + r[:-61]\n",
    "\n",
    "    # print('s bits:', h)\n",
    "    # print('')\n",
    "\n",
    "    round_key_int = 0\n",
    "    # print('init round int:', hex(round_key_int))\n",
    "    for index, ind_bit in enumerate(h):\n",
    "        round_key_int += (ind_bit << index)\n",
    "        # print('round:',index, '-', hex(round_key_int))\n",
    "\n",
    "    # print('round_key_int', hex(round_key_int))\n",
    "    # print('')\n",
    "\n",
    "    upper_nibble = round_key_int >> 76\n",
    "\n",
    "    # print('upper_nibble:', upper_nibble)\n",
    "\n",
    "    upper_nibble = s_box[upper_nibble]\n",
    "\n",
    "    # print('upper_nibble sboxed', hex(upper_nibble))\n",
    "\n",
    "    xor_portion = ((round_key_int >> 15) & 0x1F) ^ round_count\n",
    "    # print('Count:', round_count)\n",
    "    # print('XOR Value:', xor_portion)\n",
    "\n",
    "    # print('Before:', hex(round_key_int))\n",
    "    round_key_int = (round_key_int & 0x0FFFFFFFFFFFFFF07FFF) + (upper_nibble << 76) + (xor_portion << 15)\n",
    "    # print('After: ', hex(round_key_int))\n",
    "\n",
    "    return round_key_int\n",
    "\n",
    "\n",
    "\n",
    "test_vectors_80 = {1:(0x00000000000000000000, 0x0000000000000000, 0x5579C1387B228445),\n",
    "                2:(0xFFFFFFFFFFFFFFFFFFFF, 0x0000000000000000, 0xE72C46C0F5945049),\n",
    "                3:(0x00000000000000000000, 0xFFFFFFFFFFFFFFFF, 0xA112FFC72F68417B),\n",
    "                4:(0xFFFFFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF, 0x3333DCD3213210D2)}\n",
    "\n",
    "print('Testing 80-bit Key Vectors:')\n",
    "\n",
    "\n",
    "\n",
    "for test_case in test_vectors_80:\n",
    "\n",
    "    key_schedule = []\n",
    "    current_round_key = test_vectors_80[test_case][0]\n",
    "    round_state = test_vectors_80[test_case][1]\n",
    "\n",
    "    # Key schedule\n",
    "    for rnd_cnt in range(ROUND_LIMIT):\n",
    "        # print(format(round_key, '020X'))\n",
    "        # print(format(round_key >> 16, '016X'))\n",
    "        key_schedule.append(current_round_key >> 16)\n",
    "        current_round_key = key_function_80(current_round_key, rnd_cnt + 1)\n",
    "\n",
    "    for rnd in range(ROUND_LIMIT - 1):\n",
    "        # print('Round:', rnd)\n",
    "        # print('State:', format(round_state, '016X'))\n",
    "        # print('R_Key:', format(key_schedule[rnd], '016X'))\n",
    "        round_state = round_function(round_state, key_schedule[rnd])\n",
    "\n",
    "    round_state ^= key_schedule[ROUND_LIMIT-1]\n",
    "\n",
    "    if round_state == test_vectors_80[test_case][2]:\n",
    "        print('Success', hex(round_state))\n",
    "    else:\n",
    "        print('Failure', hex(round_state))\n",
    "        \n",
    "def PRESENT(P, K, ROUND):\n",
    "    key_schedule = []\n",
    "    current_round_key = K\n",
    "    round_state = P\n",
    "    \n",
    "    if(ROUND==0):\n",
    "        return P\n",
    "\n",
    "    for rnd_cnt in range(ROUND):\n",
    "        key_schedule.append(current_round_key >> 16)\n",
    "        current_round_key = key_function_80(current_round_key, rnd_cnt + 1)\n",
    "\n",
    "    for rnd in range(ROUND - 1):\n",
    "        round_state = round_function(round_state, key_schedule[rnd])\n",
    "\n",
    "    round_state ^= key_schedule[ROUND-1]\n",
    "    \n",
    "    return round_state\n",
    "\n",
    "C = PRESENT(0x0, 0x0, ROUND=32)\n",
    "print(hex(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AwZVgDHVQpAz"
   },
   "outputs": [],
   "source": [
    "Wang_diff = [0x7000000000007000, 0x0700000000000700, 0x0070000000000070, 0x0007000000000007]\n",
    "BLOCK_SIZE = 64\n",
    "\n",
    "ROUND_global = 6\n",
    "sample_num = 10000\n",
    "test_sample_num = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xJrNRizYQpA0"
   },
   "outputs": [],
   "source": [
    "def gen(sample_num, ROUND):\n",
    "    P_set = []\n",
    "    K_set = []\n",
    "    for i in range(sample_num):\n",
    "        P_set.append(random.randrange(0,2**64))\n",
    "        #print(\"%x\" % P_set[i])\n",
    "        K_set.append(random.randrange(0,2**80))\n",
    "        #print(\"%x\" % K_set[i])\n",
    "\n",
    "    C_diff_set = []\n",
    "    C_diff_label = []\n",
    "    for i in range(sample_num):\n",
    "        P = P_set[i]\n",
    "        K = K_set[i]\n",
    "        C = PRESENT(P, K, ROUND)\n",
    "        for j in range(4):\n",
    "            Cj = PRESENT(P^Wang_diff[j], K, ROUND)\n",
    "            C_diff = C^Cj\n",
    "            #print(C_diff)\n",
    "            C_diff_set.append(C_diff)\n",
    "            temp = [0, 0, 0, 0]\n",
    "            temp[j] = 1\n",
    "            C_diff_label.append(temp)\n",
    "\n",
    "    tr_X = []\n",
    "    for C_diff in C_diff_set:\n",
    "        A = []\n",
    "        for j in range(BLOCK_SIZE):\n",
    "            A.append((C_diff>>j)&1)\n",
    "        tr_X.append(A)\n",
    "        #print(A)\n",
    "    tr_X = np.array(tr_X)\n",
    "\n",
    "    tr_X = []\n",
    "    for C_diff in C_diff_set:\n",
    "        A = []\n",
    "        for j in range(BLOCK_SIZE):\n",
    "            A.append((C_diff>>j)&1)\n",
    "        tr_X.append(A)\n",
    "        #print(A)\n",
    "    tr_X = np.array(tr_X)\n",
    "    tr_t = np.array(C_diff_label)\n",
    "\n",
    "    ind = np.arange(len(tr_X))\n",
    "    np.random.shuffle(ind)\n",
    "    tr_X = tr_X[ind]\n",
    "    tr_t = tr_t[ind]\n",
    "    \n",
    "    return tr_X, tr_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gen():\n",
    "    SAMPLE_NUM_RANGE = [10000, 50000, 100000]\n",
    "    ROUND_RANGE = [3, 4, 5, 6, 7, 8]\n",
    "    for sn in SAMPLE_NUM_RANGE:\n",
    "        for rn in ROUND_RANGE:\n",
    "            tr_X, tr_t = gen(sn, rn)\n",
    "            np.save(\"ROUND %d SAMPLE %d Dataset\" % (rn, sn), tr_X)\n",
    "            np.save(\"ROUND %d SAMPLE %d Label\" % (rn, sn), tr_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Wk0Lr4ReQpA1"
   },
   "outputs": [],
   "source": [
    "def test_sample_gen():\n",
    "    TEST_SMAPLE_NUM = 10000\n",
    "    for rn in ROUND_RANGE:\n",
    "        te_X, te_t = gen(TEST_SMAPLE_NUM, rn)\n",
    "        np.save(\"ROUND %d TEST_SAMPLE Dataset\" % (rn), te_X)\n",
    "        np.save(\"ROUND %d TEST_SAMPLE Label\" % (rn), te_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O9XPTV7FQpA1"
   },
   "outputs": [],
   "source": [
    "def load_sample(SAMPLE_NUM, ROUND_NUM):\n",
    "    tr_X = np.load(\"ROUND %d SAMPLE %d Dataset.npy\" % (ROUND_NUM, SAMPLE_NUM))\n",
    "    tr_t = np.load(\"ROUND %d SAMPLE %d Label.npy\" % (ROUND_NUM, SAMPLE_NUM))\n",
    "    return tr_X, tr_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_sample(ROUND_NUM):\n",
    "    te_X = np.load(\"ROUND %d TEST_SAMPLE Dataset.npy\" % (ROUND_NUM))\n",
    "    te_t = np.load(\"ROUND %d TEST_SAMPLE Label.npy\" % (ROUND_NUM))\n",
    "    return te_X, te_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RfujwI1AQpA1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer1=128, layer2=1028, layer3=None, reg=None, learning_rate=0.001):\n",
    "        self.layers = self._build_layers(layer1, layer2, layer3, reg)\n",
    "        self.model = tf.keras.Sequential(self.layers) \n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def _build_layers(self, layer1, layer2, layer3, reg):\n",
    "        if layer3==None:\n",
    "            layers = [\n",
    "                tf.keras.layers.Flatten(input_shape=(64,)),\n",
    "                tf.keras.layers.Dense(layer1, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer2, activation='relu', kernel_regularizer=reg),\n",
    "                #tf.keras.layers.Dense(layer3, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(4, activation='softmax')\n",
    "            ]\n",
    "        else:\n",
    "            layers = [\n",
    "                tf.keras.layers.Flatten(input_shape=(64,)),\n",
    "                tf.keras.layers.Dense(layer1, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer2, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer3, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(4, activation='softmax')\n",
    "            ]\n",
    "        return layers\n",
    "\n",
    "    #그냥 cross entropy를 그대로 정의함\n",
    "    def _my_loss(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32) #float32 => int32로 casting #None, 1\n",
    "        y_true = tf.squeeze(tf.one_hot(y_true, depth=10, dtype=tf.float32), 1) # one_hot encoding #None, 1, 10 #quezze => 1을 없애줌 => None, 10\n",
    "        y_pred = tf.nn.softmax(y_pred, 1) # 한 축에 대해 softmax를 적용해라 #1 => 열을 의미 #즉, 한 행에 있는 값을 다 더하면 1이 되도록 만들어줌\n",
    "\n",
    "        #cross entropy 그대로 적용\n",
    "        #-sum t*log y 한 후에 평균 냄\n",
    "        return -tf.reduce_mean(tf.reduce_sum(\n",
    "                tf.multiply(y_true, tf.math.log(y_pred)), 1))\n",
    "\n",
    "    def _my_accuracy(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        y_true = tf.squeeze(tf.one_hot(y_true, depth=10, dtype=tf.float32), 1)\n",
    "        #argmax를 그대로 이용\n",
    "        return tf.reduce_mean(\n",
    "            tf.cast(\n",
    "                tf.equal(tf.argmax(y_true, 1), tf.argmax(y_pred, 1)), tf.float32))\n",
    "\n",
    "    def fit(self, x, t, epochs, batch_size=None, validation_split=0.0, verbose=1, shuffle=False, workers=2):\n",
    "        self.model.fit(x, t, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=verbose, shuffle=shuffle, workers=workers)\n",
    "    \n",
    "    def evaluate(self, x=None, y=None, verbose=1):\n",
    "        return self.model.evaluate(x=x, y=y, verbose=verbose)\n",
    "    \n",
    "    def summary(self):\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhQTZ7KwQpA2",
    "outputId": "85b3d255-445f-463d-ba53-122475dfd771"
   },
   "outputs": [],
   "source": [
    "#Config\n",
    "ROUND = 6\n",
    "SAMPLE_NUM = 10000\n",
    "test_sample_num = 10000\n",
    "ITERATION = 5\n",
    "\n",
    "#Fix\n",
    "batch_size = 200\n",
    "epoch_size = 25\n",
    "validation_split = 0.3\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w53eKengQpA2"
   },
   "outputs": [],
   "source": [
    "def learn(ROUND, SAMPLE_NUM, layer1, layer2, layer3=None, reg=None, learning_rate=0.001):\n",
    "    tr_X, tr_t = load_sample(SAMPLE_NUM=SAMPLE_NUM, ROUND_NUM=ROUND)\n",
    "    te_X, te_t = load_test_sample(ROUND_NUM=ROUND)\n",
    "    accuracy = []\n",
    "    for i in range(ITERATION):\n",
    "        model = MLP(layer1, layer2, layer3, reg, learning_rate)\n",
    "        model.fit(tr_X, tr_t, epochs=epoch_size, batch_size=batch_size, validation_split = validation_split, shuffle=True, verbose=0)\n",
    "        accuracy.append(model.evaluate(te_X, te_t, verbose=0)[1])\n",
    "    avg = np.mean(np.array(accuracy))\n",
    "    #print(\"average : %f\" % avg)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_reg_L2(ROUND=ROUND, l1=128, l2=512, weight=[0.0001]):\n",
    "    tr_X, tr_t = gen(sample_num, ROUND)\n",
    "    te_X, te_t = gen(test_sample_num, ROUND)\n",
    "\n",
    "    result = []\n",
    "    result_weight = []\n",
    "    for w in weight:\n",
    "        accuracy = []\n",
    "        for i in range(ITERATION):\n",
    "            model = MLP(layer1=l1, layer2=l2, reg=tf.keras.regularizers.L2(w))\n",
    "            model.fit(tr_X, tr_t, epochs=epoch_size, batch_size=batch_size, validation_split = validation_split, verbose=0)\n",
    "            accuracy.append(model.evaluate(te_X, te_t)[1])\n",
    "        avg_acc = np.mean(np.array(accuracy))\n",
    "        result.append(avg_acc)\n",
    "        result_weight.append(w)\n",
    "    return result, result_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.5342 - accuracy: 0.3875\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.5329 - accuracy: 0.3858\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.5820 - accuracy: 0.3771\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.5666 - accuracy: 0.3757\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.5530 - accuracy: 0.3834\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 1.3078 - accuracy: 0.4181\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3078 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3015 - accuracy: 0.4138\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3042 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3245 - accuracy: 0.4088\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2436 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2473 - accuracy: 0.4109\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2479 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2454 - accuracy: 0.4116\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2425 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3206 - accuracy: 0.3492\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3221 - accuracy: 0.3500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3213 - accuracy: 0.3502\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3246 - accuracy: 0.3427\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3206 - accuracy: 0.3496\n",
      "[0.3819149971008301, 0.41356500387191775, 0.41321499943733214, 0.3483449935913086]\n",
      "[0.0001, 0.001, 0.01, 0.1]\n"
     ]
    }
   ],
   "source": [
    "weight = [0.0001, 0.001, 0.01, 0.1]\n",
    "accuracy, accuracy_weight = learn_reg_L2(ROUND=5, l1=128, l2=512, weight=weight)\n",
    "maxidx = accuracy.index(max(accuracy))\n",
    "print(accuracy)\n",
    "print(accuracy_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 2s 2ms/step - loss: 1.5823 - accuracy: 0.3803\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.6351 - accuracy: 0.3846\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.6225 - accuracy: 0.3849\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.5974 - accuracy: 0.3867\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.6129 - accuracy: 0.3825\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2415 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2475 - accuracy: 0.4087\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2390 - accuracy: 0.4172\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2410 - accuracy: 0.4106\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2409 - accuracy: 0.4170\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2704 - accuracy: 0.4021\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2697 - accuracy: 0.4005\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2703 - accuracy: 0.4024\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2721 - accuracy: 0.4015\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2674 - accuracy: 0.4022\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2871 - accuracy: 0.3873\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2857 - accuracy: 0.3899\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2911 - accuracy: 0.3849\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2868 - accuracy: 0.3886\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 1.2898 - accuracy: 0.3842\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2982 - accuracy: 0.3837\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2972 - accuracy: 0.3834\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2975 - accuracy: 0.3830\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2971 - accuracy: 0.3827\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2976 - accuracy: 0.3824\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3086 - accuracy: 0.3611\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3089 - accuracy: 0.3621\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3088 - accuracy: 0.3633\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3045 - accuracy: 0.3771\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3078 - accuracy: 0.3620\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3124 - accuracy: 0.3555\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3124 - accuracy: 0.3563\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3129 - accuracy: 0.3570\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3128 - accuracy: 0.3561\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3121 - accuracy: 0.3572\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3154 - accuracy: 0.3544\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3153 - accuracy: 0.3550\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3164 - accuracy: 0.3544\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3176 - accuracy: 0.3548\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3150 - accuracy: 0.3544\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3200 - accuracy: 0.3542\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3204 - accuracy: 0.3526\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3195 - accuracy: 0.3551\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3205 - accuracy: 0.3532\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3182 - accuracy: 0.3540\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3223 - accuracy: 0.3527\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3220 - accuracy: 0.3530\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3223 - accuracy: 0.3526\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3226 - accuracy: 0.3520\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3228 - accuracy: 0.3522\n"
     ]
    }
   ],
   "source": [
    "weight = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09]\n",
    "accuracy, accuracy_weight = learn_reg_L2(ROUND=5, l1=128, l2=512, weight=weight)\n",
    "maxidx = accuracy.index(max(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005, 0.0051, 0.0052, 0.0053, 0.0054, 0.0055, 0.0056, 0.0057, 0.0058000000000000005, 0.0059, 0.006, 0.0061, 0.0062, 0.0063, 0.0064, 0.006500000000000001, 0.0066, 0.0067, 0.0068000000000000005, 0.0069, 0.007, 0.0071, 0.0072, 0.0073, 0.0074, 0.0075, 0.0076, 0.0077, 0.0078, 0.0079, 0.008, 0.0081, 0.0082, 0.0083, 0.0084, 0.0085, 0.0086, 0.0087, 0.0088, 0.0089, 0.009000000000000001, 0.0091, 0.0092, 0.0093, 0.0094, 0.0095, 0.009600000000000001, 0.0097, 0.0098, 0.009899999999999999, 0.01, 0.010100000000000001, 0.0102, 0.0103, 0.0104, 0.010499999999999999, 0.0106, 0.010700000000000001, 0.0108, 0.0109, 0.011, 0.0111, 0.0112, 0.011300000000000001, 0.0114, 0.0115, 0.0116, 0.0117, 0.0118, 0.0119, 0.012, 0.0121, 0.012199999999999999, 0.0123, 0.012400000000000001, 0.0125, 0.0126, 0.0127, 0.012799999999999999, 0.012900000000000002, 0.013000000000000001, 0.0131, 0.0132, 0.0133, 0.013399999999999999, 0.013500000000000002, 0.013600000000000001, 0.0137, 0.0138, 0.0139, 0.013999999999999999, 0.014100000000000001, 0.0142, 0.0143, 0.0144, 0.014499999999999999, 0.014599999999999998, 0.014700000000000001, 0.0148, 0.0149]\n"
     ]
    }
   ],
   "source": [
    "RANGE1 = 100\n",
    "weight2 = []\n",
    "if(maxidx==0):\n",
    "    base = 0\n",
    "else:\n",
    "    base = maxidx*0.01 - 0.005\n",
    "for i in range(RANGE1):\n",
    "    weight2.append(base + (i/10000.0))\n",
    "print(weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MVTYS1K6uVn2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2376 - accuracy: 0.4189\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2343 - accuracy: 0.4189\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2391 - accuracy: 0.4188\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2395 - accuracy: 0.4177\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2389 - accuracy: 0.4182\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2404 - accuracy: 0.4197\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2420 - accuracy: 0.4201\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2427 - accuracy: 0.4154\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2332 - accuracy: 0.4204\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2380 - accuracy: 0.4167\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2395 - accuracy: 0.4179\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2383 - accuracy: 0.4167\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2382 - accuracy: 0.4190\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2458 - accuracy: 0.4169\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2401 - accuracy: 0.4213\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2367 - accuracy: 0.4213\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2388 - accuracy: 0.4189\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2371 - accuracy: 0.4195\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2419 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2372 - accuracy: 0.4217\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2349 - accuracy: 0.4228\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2373 - accuracy: 0.4189\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2359 - accuracy: 0.4210\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2420 - accuracy: 0.4185\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2411 - accuracy: 0.4187\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2402 - accuracy: 0.4190\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2367 - accuracy: 0.4194\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2425 - accuracy: 0.4184\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2369 - accuracy: 0.4184\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2307 - accuracy: 0.4226\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2364 - accuracy: 0.4184\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2431 - accuracy: 0.4161\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2362 - accuracy: 0.4202\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2392 - accuracy: 0.4200\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2414 - accuracy: 0.4190\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2376 - accuracy: 0.4178\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2427 - accuracy: 0.4175\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2413 - accuracy: 0.4168\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2327 - accuracy: 0.4234\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2373 - accuracy: 0.4149\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2353 - accuracy: 0.4201\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2361 - accuracy: 0.4218\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2458 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2430 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2410 - accuracy: 0.4197\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2399 - accuracy: 0.4161\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2369 - accuracy: 0.4202\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2374 - accuracy: 0.4202\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2432 - accuracy: 0.4175\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2386 - accuracy: 0.4198\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2358 - accuracy: 0.4198\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2402 - accuracy: 0.4195\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2420 - accuracy: 0.4190\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2399 - accuracy: 0.4185\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2335 - accuracy: 0.4206\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2404 - accuracy: 0.4194\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2377 - accuracy: 0.4191\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2372 - accuracy: 0.4192\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2371 - accuracy: 0.4222\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2404 - accuracy: 0.4163\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2404 - accuracy: 0.4193\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2430 - accuracy: 0.4168\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2370 - accuracy: 0.4198\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2367 - accuracy: 0.4190\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2397 - accuracy: 0.4190\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2365 - accuracy: 0.4217\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2455 - accuracy: 0.4180\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2398 - accuracy: 0.4187\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2396 - accuracy: 0.4191\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2404 - accuracy: 0.4188\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2336 - accuracy: 0.4223\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2384 - accuracy: 0.4212\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2386 - accuracy: 0.4198\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2415 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2372 - accuracy: 0.4190\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2410 - accuracy: 0.4184\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2398 - accuracy: 0.4189\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2366 - accuracy: 0.4177\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2445 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2382 - accuracy: 0.4167\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2380 - accuracy: 0.4211\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2376 - accuracy: 0.4215\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2403 - accuracy: 0.4173\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2353 - accuracy: 0.4216\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2414 - accuracy: 0.4213\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2419 - accuracy: 0.4183\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2377 - accuracy: 0.4183\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2429 - accuracy: 0.4165\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2387 - accuracy: 0.4183\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2453 - accuracy: 0.4179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2410 - accuracy: 0.4168\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2420 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2405 - accuracy: 0.4171\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2408 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.2434 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2406 - accuracy: 0.4187\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2393 - accuracy: 0.4186\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.2364 - accuracy: 0.41 - 5s 4ms/step - loss: 1.2365 - accuracy: 0.4191\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2457 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2412 - accuracy: 0.4162\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2411 - accuracy: 0.4166\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2391 - accuracy: 0.4186\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2362 - accuracy: 0.4199\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2407 - accuracy: 0.4172\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2420 - accuracy: 0.4175\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2452 - accuracy: 0.4159\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2432 - accuracy: 0.4163\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2381 - accuracy: 0.4203\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2401 - accuracy: 0.4185\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2423 - accuracy: 0.4169\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2419 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2432 - accuracy: 0.4173\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2383 - accuracy: 0.4185\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2344 - accuracy: 0.4189\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2377 - accuracy: 0.4171\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2402 - accuracy: 0.4189\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2407 - accuracy: 0.4162\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2449 - accuracy: 0.4189\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2421 - accuracy: 0.4177\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2444 - accuracy: 0.4165\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2386 - accuracy: 0.4170\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2410 - accuracy: 0.4168\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2430 - accuracy: 0.4174\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2398 - accuracy: 0.4212\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2399 - accuracy: 0.4196\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2402 - accuracy: 0.4182\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2415 - accuracy: 0.4179\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2385 - accuracy: 0.4162\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2427 - accuracy: 0.4154\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2387 - accuracy: 0.4185\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2364 - accuracy: 0.4213\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2421 - accuracy: 0.4186\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2409 - accuracy: 0.4183\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2393 - accuracy: 0.4178\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2409 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2422 - accuracy: 0.4171\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2446 - accuracy: 0.4138\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2387 - accuracy: 0.4187\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2374 - accuracy: 0.4196\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2393 - accuracy: 0.4189\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2391 - accuracy: 0.4180\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2420 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2339 - accuracy: 0.4191\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2426 - accuracy: 0.4166\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2397 - accuracy: 0.4180\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2431 - accuracy: 0.4171\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2405 - accuracy: 0.4184\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2418 - accuracy: 0.4168\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2417 - accuracy: 0.4162\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2433 - accuracy: 0.4147\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2428 - accuracy: 0.4171\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2441 - accuracy: 0.4166\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2409 - accuracy: 0.4178\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2482 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2423 - accuracy: 0.4183\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2378 - accuracy: 0.4201\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2384 - accuracy: 0.4193\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2433 - accuracy: 0.4147\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2427 - accuracy: 0.4162\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2393 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2393 - accuracy: 0.4172\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2417 - accuracy: 0.4178\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2414 - accuracy: 0.4169\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2444 - accuracy: 0.4161\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2402 - accuracy: 0.4172\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2394 - accuracy: 0.4182\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2413 - accuracy: 0.4191\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2480 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2404 - accuracy: 0.4167\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2409 - accuracy: 0.4178\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2403 - accuracy: 0.4178\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2424 - accuracy: 0.4179\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2426 - accuracy: 0.4156\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2406 - accuracy: 0.4176\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2411 - accuracy: 0.4161\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2432 - accuracy: 0.4158\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2426 - accuracy: 0.4161\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2438 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2395 - accuracy: 0.4177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2418 - accuracy: 0.4182\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2440 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2441 - accuracy: 0.4154\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2426 - accuracy: 0.4172\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2416 - accuracy: 0.4187\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2379 - accuracy: 0.4196\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.2454 - accuracy: 0.41 - 5s 4ms/step - loss: 1.2456 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2430 - accuracy: 0.4166\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2414 - accuracy: 0.4189\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2433 - accuracy: 0.4177\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2388 - accuracy: 0.4167\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2457 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2478 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2445 - accuracy: 0.4136\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2396 - accuracy: 0.4170\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2415 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 1.2476 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2453 - accuracy: 0.4145\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2433 - accuracy: 0.4167\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2413 - accuracy: 0.4172\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2414 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2440 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2421 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2431 - accuracy: 0.4162\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2434 - accuracy: 0.4147\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2477 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2399 - accuracy: 0.4170\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2396 - accuracy: 0.4169\n",
      "1250/1250 [==============================] - 1s 978us/step - loss: 1.2461 - accuracy: 0.4169\n",
      "1250/1250 [==============================] - 1s 977us/step - loss: 1.2442 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 900us/step - loss: 1.2467 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 945us/step - loss: 1.2453 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 1s 925us/step - loss: 1.2490 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 994us/step - loss: 1.2422 - accuracy: 0.4170\n",
      "1250/1250 [==============================] - 1s 920us/step - loss: 1.2440 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 1s 924us/step - loss: 1.2480 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 918us/step - loss: 1.2440 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 954us/step - loss: 1.2444 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 941us/step - loss: 1.2464 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 926us/step - loss: 1.2471 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2405 - accuracy: 0.4166\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2451 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 976us/step - loss: 1.2479 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 920us/step - loss: 1.2417 - accuracy: 0.4165\n",
      "1250/1250 [==============================] - 1s 925us/step - loss: 1.2434 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 912us/step - loss: 1.2467 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 1.2448 - accuracy: 0.4126\n",
      "1250/1250 [==============================] - 1s 917us/step - loss: 1.2458 - accuracy: 0.4159\n",
      "1250/1250 [==============================] - 1s 897us/step - loss: 1.2459 - accuracy: 0.4165\n",
      "1250/1250 [==============================] - 1s 851us/step - loss: 1.2471 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 851us/step - loss: 1.2448 - accuracy: 0.4163\n",
      "1250/1250 [==============================] - 1s 831us/step - loss: 1.2433 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 811us/step - loss: 1.2465 - accuracy: 0.4158\n",
      "1250/1250 [==============================] - 1s 809us/step - loss: 1.2477 - accuracy: 0.4111\n",
      "1250/1250 [==============================] - 1s 828us/step - loss: 1.2484 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 838us/step - loss: 1.2417 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 850us/step - loss: 1.2460 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 896us/step - loss: 1.2475 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2469 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 1.2415 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 926us/step - loss: 1.2482 - accuracy: 0.4089\n",
      "1250/1250 [==============================] - 1s 935us/step - loss: 1.2468 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 969us/step - loss: 1.2449 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2470 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 924us/step - loss: 1.2440 - accuracy: 0.4138\n",
      "1250/1250 [==============================] - 1s 918us/step - loss: 1.2474 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 951us/step - loss: 1.2483 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 941us/step - loss: 1.2481 - accuracy: 0.4088\n",
      "1250/1250 [==============================] - 1s 929us/step - loss: 1.2500 - accuracy: 0.4108\n",
      "1250/1250 [==============================] - 1s 959us/step - loss: 1.2466 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 973us/step - loss: 1.2455 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 941us/step - loss: 1.2482 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 919us/step - loss: 1.2456 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 1s 945us/step - loss: 1.2433 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 1s 963us/step - loss: 1.2440 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 945us/step - loss: 1.2439 - accuracy: 0.4152\n",
      "1250/1250 [==============================] - 1s 929us/step - loss: 1.2455 - accuracy: 0.4145\n",
      "1250/1250 [==============================] - 1s 982us/step - loss: 1.2432 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 937us/step - loss: 1.2444 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 940us/step - loss: 1.2449 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 956us/step - loss: 1.2456 - accuracy: 0.4147\n",
      "1250/1250 [==============================] - 1s 963us/step - loss: 1.2466 - accuracy: 0.4113\n",
      "1250/1250 [==============================] - 1s 923us/step - loss: 1.2452 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 926us/step - loss: 1.2486 - accuracy: 0.4145\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2448 - accuracy: 0.4161\n",
      "1250/1250 [==============================] - 1s 967us/step - loss: 1.2469 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 911us/step - loss: 1.2467 - accuracy: 0.4096\n",
      "1250/1250 [==============================] - 1s 905us/step - loss: 1.2508 - accuracy: 0.4077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 952us/step - loss: 1.2466 - accuracy: 0.4159\n",
      "1250/1250 [==============================] - 1s 899us/step - loss: 1.2447 - accuracy: 0.4149\n",
      "1250/1250 [==============================] - 1s 884us/step - loss: 1.2506 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 1.2476 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 865us/step - loss: 1.2501 - accuracy: 0.4116\n",
      "1250/1250 [==============================] - 1s 832us/step - loss: 1.2448 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 1s 832us/step - loss: 1.2501 - accuracy: 0.4125\n",
      "1250/1250 [==============================] - 1s 840us/step - loss: 1.2502 - accuracy: 0.4088\n",
      "1250/1250 [==============================] - 1s 808us/step - loss: 1.2470 - accuracy: 0.4122\n",
      "1250/1250 [==============================] - 1s 809us/step - loss: 1.2465 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 819us/step - loss: 1.2488 - accuracy: 0.4108\n",
      "1250/1250 [==============================] - 1s 860us/step - loss: 1.2488 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 815us/step - loss: 1.2495 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 1s 828us/step - loss: 1.2476 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 1.2453 - accuracy: 0.4105\n",
      "1250/1250 [==============================] - 1s 888us/step - loss: 1.2436 - accuracy: 0.4159\n",
      "1250/1250 [==============================] - 1s 908us/step - loss: 1.2490 - accuracy: 0.4115\n",
      "1250/1250 [==============================] - 1s 916us/step - loss: 1.2537 - accuracy: 0.4094\n",
      "1250/1250 [==============================] - 1s 971us/step - loss: 1.2461 - accuracy: 0.4103\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 1.2488 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2477 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 971us/step - loss: 1.2442 - accuracy: 0.4097\n",
      "1250/1250 [==============================] - 1s 953us/step - loss: 1.2460 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 936us/step - loss: 1.2483 - accuracy: 0.4114\n",
      "1250/1250 [==============================] - 1s 936us/step - loss: 1.2456 - accuracy: 0.4104\n",
      "1250/1250 [==============================] - 1s 971us/step - loss: 1.2494 - accuracy: 0.4095\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2457 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 935us/step - loss: 1.2477 - accuracy: 0.4118\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 1.2507 - accuracy: 0.4095\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2503 - accuracy: 0.4097\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 1.2497 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 920us/step - loss: 1.2482 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 920us/step - loss: 1.2507 - accuracy: 0.4092\n",
      "1250/1250 [==============================] - 1s 951us/step - loss: 1.2502 - accuracy: 0.4116\n",
      "1250/1250 [==============================] - 1s 922us/step - loss: 1.2467 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 938us/step - loss: 1.2473 - accuracy: 0.4118\n",
      "1250/1250 [==============================] - 1s 959us/step - loss: 1.2496 - accuracy: 0.4103\n",
      "1250/1250 [==============================] - 1s 956us/step - loss: 1.2501 - accuracy: 0.4087\n",
      "1250/1250 [==============================] - 1s 920us/step - loss: 1.2470 - accuracy: 0.4147\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2490 - accuracy: 0.4107\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 1.2452 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 1s 912us/step - loss: 1.2507 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 903us/step - loss: 1.2485 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 892us/step - loss: 1.2551 - accuracy: 0.4081\n",
      "1250/1250 [==============================] - 1s 865us/step - loss: 1.2502 - accuracy: 0.4090\n",
      "1250/1250 [==============================] - 1s 856us/step - loss: 1.2491 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 898us/step - loss: 1.2514 - accuracy: 0.4093\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 1.2504 - accuracy: 0.4147\n",
      "1250/1250 [==============================] - 1s 823us/step - loss: 1.2491 - accuracy: 0.4131\n",
      "1250/1250 [==============================] - 1s 810us/step - loss: 1.2502 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 848us/step - loss: 1.2471 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 814us/step - loss: 1.2504 - accuracy: 0.4079\n",
      "1250/1250 [==============================] - 1s 817us/step - loss: 1.2488 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 848us/step - loss: 1.2485 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 1.2509 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 891us/step - loss: 1.2478 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 911us/step - loss: 1.2495 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2505 - accuracy: 0.4090\n",
      "1250/1250 [==============================] - 1s 918us/step - loss: 1.2483 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 913us/step - loss: 1.2489 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 996us/step - loss: 1.2473 - accuracy: 0.4114\n",
      "1250/1250 [==============================] - 1s 947us/step - loss: 1.2451 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 944us/step - loss: 1.2510 - accuracy: 0.4103\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2509 - accuracy: 0.4074\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2485 - accuracy: 0.4145\n",
      "1250/1250 [==============================] - 1s 926us/step - loss: 1.2514 - accuracy: 0.4087\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 1.2475 - accuracy: 0.4149\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2517 - accuracy: 0.4102\n",
      "1250/1250 [==============================] - 1s 929us/step - loss: 1.2468 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 1.2487 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 937us/step - loss: 1.2510 - accuracy: 0.4116\n",
      "1250/1250 [==============================] - 1s 941us/step - loss: 1.2501 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 935us/step - loss: 1.2512 - accuracy: 0.4096\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2534 - accuracy: 0.4086\n",
      "1250/1250 [==============================] - 1s 963us/step - loss: 1.2512 - accuracy: 0.4098\n",
      "1250/1250 [==============================] - 1s 936us/step - loss: 1.2498 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 937us/step - loss: 1.2519 - accuracy: 0.4109\n",
      "1250/1250 [==============================] - 1s 964us/step - loss: 1.2501 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 939us/step - loss: 1.2495 - accuracy: 0.4107\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 1.2496 - accuracy: 0.4105\n",
      "1250/1250 [==============================] - 1s 936us/step - loss: 1.2503 - accuracy: 0.4097\n",
      "1250/1250 [==============================] - 1s 968us/step - loss: 1.2524 - accuracy: 0.4060\n",
      "1250/1250 [==============================] - 1s 954us/step - loss: 1.2576 - accuracy: 0.4053\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2517 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 1.2521 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2511 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 955us/step - loss: 1.2494 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 891us/step - loss: 1.2537 - accuracy: 0.4097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 870us/step - loss: 1.2512 - accuracy: 0.4089\n",
      "1250/1250 [==============================] - 1s 843us/step - loss: 1.2536 - accuracy: 0.4103\n",
      "1250/1250 [==============================] - 1s 843us/step - loss: 1.2487 - accuracy: 0.4118\n",
      "1250/1250 [==============================] - 1s 832us/step - loss: 1.2535 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 810us/step - loss: 1.2519 - accuracy: 0.4081\n",
      "1250/1250 [==============================] - 1s 822us/step - loss: 1.2531 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 812us/step - loss: 1.2531 - accuracy: 0.4105\n",
      "1250/1250 [==============================] - 1s 836us/step - loss: 1.2521 - accuracy: 0.4108\n",
      "1250/1250 [==============================] - 1s 836us/step - loss: 1.2522 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 846us/step - loss: 1.2521 - accuracy: 0.4113\n",
      "1250/1250 [==============================] - 1s 903us/step - loss: 1.2526 - accuracy: 0.4072\n",
      "1250/1250 [==============================] - 1s 966us/step - loss: 1.2515 - accuracy: 0.4109\n",
      "1250/1250 [==============================] - 1s 937us/step - loss: 1.2520 - accuracy: 0.4131\n",
      "1250/1250 [==============================] - 1s 946us/step - loss: 1.2539 - accuracy: 0.4096\n",
      "1250/1250 [==============================] - 1s 964us/step - loss: 1.2518 - accuracy: 0.4094\n",
      "1250/1250 [==============================] - 1s 941us/step - loss: 1.2539 - accuracy: 0.4070\n",
      "1250/1250 [==============================] - 1s 919us/step - loss: 1.2481 - accuracy: 0.4115\n",
      "1250/1250 [==============================] - 1s 954us/step - loss: 1.2505 - accuracy: 0.4097\n",
      "1250/1250 [==============================] - 1s 942us/step - loss: 1.2510 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 928us/step - loss: 1.2526 - accuracy: 0.4093\n",
      "1250/1250 [==============================] - 1s 962us/step - loss: 1.2534 - accuracy: 0.4031\n",
      "1250/1250 [==============================] - 1s 937us/step - loss: 1.2532 - accuracy: 0.4087\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2523 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2498 - accuracy: 0.4098\n",
      "1250/1250 [==============================] - 1s 950us/step - loss: 1.2513 - accuracy: 0.4100\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2517 - accuracy: 0.4108\n",
      "1250/1250 [==============================] - 1s 935us/step - loss: 1.2502 - accuracy: 0.4106\n",
      "1250/1250 [==============================] - 1s 968us/step - loss: 1.2557 - accuracy: 0.4060\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 1.2518 - accuracy: 0.4086\n",
      "1250/1250 [==============================] - 1s 921us/step - loss: 1.2543 - accuracy: 0.4085\n",
      "1250/1250 [==============================] - 1s 942us/step - loss: 1.2508 - accuracy: 0.4100\n",
      "1250/1250 [==============================] - 1s 893us/step - loss: 1.2552 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 901us/step - loss: 1.2537 - accuracy: 0.4088\n",
      "1250/1250 [==============================] - 1s 891us/step - loss: 1.2548 - accuracy: 0.4094\n",
      "1250/1250 [==============================] - 1s 894us/step - loss: 1.2534 - accuracy: 0.4082\n",
      "1250/1250 [==============================] - 1s 862us/step - loss: 1.2564 - accuracy: 0.4097\n",
      "1250/1250 [==============================] - 1s 846us/step - loss: 1.2513 - accuracy: 0.4111\n",
      "1250/1250 [==============================] - 1s 833us/step - loss: 1.2507 - accuracy: 0.4109\n",
      "1250/1250 [==============================] - 1s 852us/step - loss: 1.2524 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 896us/step - loss: 1.2542 - accuracy: 0.4065\n",
      "1250/1250 [==============================] - 1s 897us/step - loss: 1.2532 - accuracy: 0.4086\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 1.2514 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 950us/step - loss: 1.2558 - accuracy: 0.4076\n",
      "1250/1250 [==============================] - 1s 928us/step - loss: 1.2540 - accuracy: 0.4076\n",
      "1250/1250 [==============================] - 1s 938us/step - loss: 1.2571 - accuracy: 0.4051\n",
      "1250/1250 [==============================] - 1s 953us/step - loss: 1.2535 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 940us/step - loss: 1.2586 - accuracy: 0.4076\n",
      "1250/1250 [==============================] - 1s 986us/step - loss: 1.2549 - accuracy: 0.4095\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2560 - accuracy: 0.4057\n",
      "1250/1250 [==============================] - 1s 926us/step - loss: 1.2522 - accuracy: 0.4092\n",
      "1250/1250 [==============================] - 1s 972us/step - loss: 1.2540 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 957us/step - loss: 1.2534 - accuracy: 0.4105\n",
      "1250/1250 [==============================] - 1s 939us/step - loss: 1.2537 - accuracy: 0.4092\n",
      "1250/1250 [==============================] - 1s 948us/step - loss: 1.2551 - accuracy: 0.4075\n",
      "1250/1250 [==============================] - 1s 942us/step - loss: 1.2545 - accuracy: 0.4072\n",
      "1250/1250 [==============================] - 1s 932us/step - loss: 1.2523 - accuracy: 0.4104\n",
      "1250/1250 [==============================] - 1s 959us/step - loss: 1.2540 - accuracy: 0.4073\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2561 - accuracy: 0.4056\n",
      "1250/1250 [==============================] - 1s 926us/step - loss: 1.2520 - accuracy: 0.4076\n",
      "1250/1250 [==============================] - 1s 996us/step - loss: 1.2533 - accuracy: 0.4131\n",
      "1250/1250 [==============================] - 1s 945us/step - loss: 1.2554 - accuracy: 0.4084\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2540 - accuracy: 0.4100\n",
      "1250/1250 [==============================] - 1s 995us/step - loss: 1.2565 - accuracy: 0.4102\n",
      "1250/1250 [==============================] - 1s 939us/step - loss: 1.2552 - accuracy: 0.4054\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 1.2537 - accuracy: 0.4084\n",
      "1250/1250 [==============================] - 1s 961us/step - loss: 1.2543 - accuracy: 0.4063\n",
      "1250/1250 [==============================] - 1s 921us/step - loss: 1.2541 - accuracy: 0.4053\n",
      "1250/1250 [==============================] - 1s 914us/step - loss: 1.2539 - accuracy: 0.4092\n",
      "1250/1250 [==============================] - 1s 909us/step - loss: 1.2532 - accuracy: 0.4101\n",
      "1250/1250 [==============================] - 1s 864us/step - loss: 1.2544 - accuracy: 0.4086\n",
      "1250/1250 [==============================] - 1s 837us/step - loss: 1.2513 - accuracy: 0.4089\n",
      "1250/1250 [==============================] - 1s 871us/step - loss: 1.2565 - accuracy: 0.4074\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 1.2543 - accuracy: 0.4089\n",
      "1250/1250 [==============================] - 1s 886us/step - loss: 1.2538 - accuracy: 0.4079\n",
      "1250/1250 [==============================] - 1s 855us/step - loss: 1.2542 - accuracy: 0.4076\n",
      "1250/1250 [==============================] - 1s 834us/step - loss: 1.2553 - accuracy: 0.4080\n",
      "1250/1250 [==============================] - 1s 847us/step - loss: 1.2538 - accuracy: 0.4042\n",
      "1250/1250 [==============================] - 1s 836us/step - loss: 1.2575 - accuracy: 0.4061\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 1.2575 - accuracy: 0.4052\n",
      "1250/1250 [==============================] - 1s 926us/step - loss: 1.2563 - accuracy: 0.4069\n",
      "1250/1250 [==============================] - 1s 929us/step - loss: 1.2569 - accuracy: 0.4092\n",
      "1250/1250 [==============================] - 1s 926us/step - loss: 1.2551 - accuracy: 0.4094\n",
      "1250/1250 [==============================] - 1s 953us/step - loss: 1.2555 - accuracy: 0.4058\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2550 - accuracy: 0.4107\n",
      "1250/1250 [==============================] - 1s 959us/step - loss: 1.2567 - accuracy: 0.4086\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 1.2586 - accuracy: 0.4050\n",
      "1250/1250 [==============================] - 1s 937us/step - loss: 1.2553 - accuracy: 0.4074\n",
      "1250/1250 [==============================] - 1s 968us/step - loss: 1.2557 - accuracy: 0.4093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 938us/step - loss: 1.2601 - accuracy: 0.4043\n",
      "1250/1250 [==============================] - 1s 940us/step - loss: 1.2547 - accuracy: 0.4100\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 1.2543 - accuracy: 0.4094\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 1.2554 - accuracy: 0.4092\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 1.2563 - accuracy: 0.4096\n",
      "1250/1250 [==============================] - 1s 973us/step - loss: 1.2548 - accuracy: 0.4080\n",
      "1250/1250 [==============================] - 1s 922us/step - loss: 1.2550 - accuracy: 0.4062\n",
      "1250/1250 [==============================] - 1s 950us/step - loss: 1.2547 - accuracy: 0.4056\n",
      "1250/1250 [==============================] - 1s 945us/step - loss: 1.2621 - accuracy: 0.4000\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2541 - accuracy: 0.4096\n",
      "1250/1250 [==============================] - 1s 948us/step - loss: 1.2559 - accuracy: 0.4069\n",
      "1250/1250 [==============================] - 1s 948us/step - loss: 1.2565 - accuracy: 0.4062\n",
      "1250/1250 [==============================] - 1s 939us/step - loss: 1.2577 - accuracy: 0.4064\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2537 - accuracy: 0.4087\n",
      "1250/1250 [==============================] - 1s 939us/step - loss: 1.2591 - accuracy: 0.4068\n",
      "1250/1250 [==============================] - 1s 937us/step - loss: 1.2563 - accuracy: 0.4072\n",
      "1250/1250 [==============================] - 1s 816us/step - loss: 1.2570 - accuracy: 0.4070\n",
      "1250/1250 [==============================] - 1s 802us/step - loss: 1.2543 - accuracy: 0.4075\n",
      "1250/1250 [==============================] - 1s 793us/step - loss: 1.2556 - accuracy: 0.4053\n",
      "1250/1250 [==============================] - 1s 928us/step - loss: 1.2553 - accuracy: 0.4102\n",
      "1250/1250 [==============================] - 1s 994us/step - loss: 1.2634 - accuracy: 0.4024\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2558 - accuracy: 0.4087\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2587 - accuracy: 0.4054\n",
      "1250/1250 [==============================] - 1s 988us/step - loss: 1.2585 - accuracy: 0.4042\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2560 - accuracy: 0.4089\n",
      "1250/1250 [==============================] - 1s 974us/step - loss: 1.2584 - accuracy: 0.4053\n",
      "1250/1250 [==============================] - 1s 949us/step - loss: 1.2568 - accuracy: 0.4067\n",
      "1250/1250 [==============================] - 1s 935us/step - loss: 1.2579 - accuracy: 0.4051\n",
      "1250/1250 [==============================] - 1s 968us/step - loss: 1.2574 - accuracy: 0.4072\n",
      "1250/1250 [==============================] - 1s 953us/step - loss: 1.2554 - accuracy: 0.4081\n",
      "1250/1250 [==============================] - 1s 954us/step - loss: 1.2531 - accuracy: 0.4098\n",
      "1250/1250 [==============================] - 1s 928us/step - loss: 1.2570 - accuracy: 0.4080\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 1.2566 - accuracy: 0.4081\n",
      "1250/1250 [==============================] - 1s 956us/step - loss: 1.2562 - accuracy: 0.4073\n",
      "1250/1250 [==============================] - 1s 914us/step - loss: 1.2587 - accuracy: 0.4061\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 1.2583 - accuracy: 0.4049\n",
      "1250/1250 [==============================] - 1s 928us/step - loss: 1.2591 - accuracy: 0.4051\n",
      "1250/1250 [==============================] - 1s 918us/step - loss: 1.2582 - accuracy: 0.4072\n",
      "1250/1250 [==============================] - 1s 895us/step - loss: 1.2598 - accuracy: 0.4027\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 1.2617 - accuracy: 0.4008\n",
      "1250/1250 [==============================] - 1s 852us/step - loss: 1.2585 - accuracy: 0.4026\n",
      "1250/1250 [==============================] - 1s 829us/step - loss: 1.2626 - accuracy: 0.4003\n",
      "1250/1250 [==============================] - 1s 853us/step - loss: 1.2571 - accuracy: 0.4066\n",
      "1250/1250 [==============================] - 1s 866us/step - loss: 1.2636 - accuracy: 0.4031\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 1.2568 - accuracy: 0.4091\n",
      "1250/1250 [==============================] - 1s 900us/step - loss: 1.2565 - accuracy: 0.4077\n",
      "1250/1250 [==============================] - 1s 950us/step - loss: 1.2587 - accuracy: 0.4057\n",
      "1250/1250 [==============================] - 1s 942us/step - loss: 1.2583 - accuracy: 0.4054\n",
      "1250/1250 [==============================] - 1s 988us/step - loss: 1.2565 - accuracy: 0.4096\n",
      "1250/1250 [==============================] - 1s 940us/step - loss: 1.2558 - accuracy: 0.4090\n",
      "1250/1250 [==============================] - 1s 928us/step - loss: 1.2598 - accuracy: 0.4045\n",
      "1250/1250 [==============================] - 1s 978us/step - loss: 1.2568 - accuracy: 0.4047\n",
      "1250/1250 [==============================] - 1s 940us/step - loss: 1.2567 - accuracy: 0.4070\n",
      "1250/1250 [==============================] - 1s 986us/step - loss: 1.2566 - accuracy: 0.4065\n",
      "1250/1250 [==============================] - 1s 950us/step - loss: 1.2595 - accuracy: 0.4041\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2557 - accuracy: 0.4078\n",
      "1250/1250 [==============================] - 1s 951us/step - loss: 1.2599 - accuracy: 0.4032\n"
     ]
    }
   ],
   "source": [
    "accuracy, accuracy_weight = learn_reg_L2(5, 128, 512, weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max weight idx : 16\n",
      "max weight : 0.006600\n",
      "max accuracy : 0.420540\n"
     ]
    }
   ],
   "source": [
    "maxidx1 = accuracy.index(max(accuracy))\n",
    "print(\"max weight idx : %d\" % maxidx1)\n",
    "print(\"max weight : %f\" % accuracy_weight[maxidx1])\n",
    "print(\"max accuracy : %f\" % accuracy[maxidx1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1dd11dbf7f0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(RANGE1)], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0066\n",
      "[0.0065   0.006501 0.006502 0.006503 0.006504 0.006505 0.006506 0.006507\n",
      " 0.006508 0.006509 0.00651  0.006511 0.006512 0.006513 0.006514 0.006515\n",
      " 0.006516 0.006517 0.006518 0.006519 0.00652  0.006521 0.006522 0.006523\n",
      " 0.006524 0.006525 0.006526 0.006527 0.006528 0.006529 0.00653  0.006531\n",
      " 0.006532 0.006533 0.006534 0.006535 0.006536 0.006537 0.006538 0.006539\n",
      " 0.00654  0.006541 0.006542 0.006543 0.006544 0.006545 0.006546 0.006547\n",
      " 0.006548 0.006549 0.00655  0.006551 0.006552 0.006553 0.006554 0.006555\n",
      " 0.006556 0.006557 0.006558 0.006559 0.00656  0.006561 0.006562 0.006563\n",
      " 0.006564 0.006565 0.006566 0.006567 0.006568 0.006569 0.00657  0.006571\n",
      " 0.006572 0.006573 0.006574 0.006575 0.006576 0.006577 0.006578 0.006579\n",
      " 0.00658  0.006581 0.006582 0.006583 0.006584 0.006585 0.006586 0.006587\n",
      " 0.006588 0.006589 0.00659  0.006591 0.006592 0.006593 0.006594 0.006595\n",
      " 0.006596 0.006597 0.006598 0.006599 0.0066   0.006601 0.006602 0.006603\n",
      " 0.006604 0.006605 0.006606 0.006607 0.006608 0.006609 0.00661  0.006611\n",
      " 0.006612 0.006613 0.006614 0.006615 0.006616 0.006617 0.006618 0.006619\n",
      " 0.00662  0.006621 0.006622 0.006623 0.006624 0.006625 0.006626 0.006627\n",
      " 0.006628 0.006629 0.00663  0.006631 0.006632 0.006633 0.006634 0.006635\n",
      " 0.006636 0.006637 0.006638 0.006639 0.00664  0.006641 0.006642 0.006643\n",
      " 0.006644 0.006645 0.006646 0.006647 0.006648 0.006649 0.00665  0.006651\n",
      " 0.006652 0.006653 0.006654 0.006655 0.006656 0.006657 0.006658 0.006659\n",
      " 0.00666  0.006661 0.006662 0.006663 0.006664 0.006665 0.006666 0.006667\n",
      " 0.006668 0.006669 0.00667  0.006671 0.006672 0.006673 0.006674 0.006675\n",
      " 0.006676 0.006677 0.006678 0.006679 0.00668  0.006681 0.006682 0.006683\n",
      " 0.006684 0.006685 0.006686 0.006687 0.006688 0.006689 0.00669  0.006691\n",
      " 0.006692 0.006693 0.006694 0.006695 0.006696 0.006697 0.006698 0.006699\n",
      " 0.0067  ]\n"
     ]
    }
   ],
   "source": [
    "unit = 1/10000.0\n",
    "start = accuracy_weight[maxidx1] + (-1)*unit\n",
    "end = accuracy_weight[maxidx1] + (+1)*unit\n",
    "step_num = 100\n",
    "step = unit/step_num\n",
    "weight2 = np.arange(start, end, step)\n",
    "print(accuracy_weight[maxidx1])\n",
    "print(weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 996us/step - loss: 1.2459 - accuracy: 0.4118\n",
      "1250/1250 [==============================] - 1s 913us/step - loss: 1.2449 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 968us/step - loss: 1.2428 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 943us/step - loss: 1.2452 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2434 - accuracy: 0.4125\n",
      "1250/1250 [==============================] - 1s 974us/step - loss: 1.2460 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 964us/step - loss: 1.2382 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 1s 948us/step - loss: 1.2484 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 936us/step - loss: 1.2448 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 978us/step - loss: 1.2454 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 953us/step - loss: 1.2429 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 987us/step - loss: 1.2471 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 946us/step - loss: 1.2427 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 947us/step - loss: 1.2466 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2436 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 1.2446 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 972us/step - loss: 1.2417 - accuracy: 0.4173\n",
      "1250/1250 [==============================] - 1s 947us/step - loss: 1.2479 - accuracy: 0.4105\n",
      "1250/1250 [==============================] - 1s 968us/step - loss: 1.2506 - accuracy: 0.4125\n",
      "1250/1250 [==============================] - 1s 951us/step - loss: 1.2420 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 968us/step - loss: 1.2437 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 942us/step - loss: 1.2432 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 942us/step - loss: 1.2413 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2416 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 946us/step - loss: 1.2453 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2432 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 951us/step - loss: 1.2444 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 947us/step - loss: 1.2471 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 954us/step - loss: 1.2421 - accuracy: 0.4122\n",
      "1250/1250 [==============================] - 1s 939us/step - loss: 1.2437 - accuracy: 0.4125\n",
      "1250/1250 [==============================] - 1s 986us/step - loss: 1.2459 - accuracy: 0.4131\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2507 - accuracy: 0.4105\n",
      "1250/1250 [==============================] - 1s 989us/step - loss: 1.2489 - accuracy: 0.4097\n",
      "1250/1250 [==============================] - 1s 963us/step - loss: 1.2435 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 991us/step - loss: 1.2444 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 942us/step - loss: 1.2475 - accuracy: 0.4122\n",
      "1250/1250 [==============================] - 1s 940us/step - loss: 1.2452 - accuracy: 0.4118\n",
      "1250/1250 [==============================] - 1s 944us/step - loss: 1.2387 - accuracy: 0.4161\n",
      "1250/1250 [==============================] - 1s 914us/step - loss: 1.2501 - accuracy: 0.4084\n",
      "1250/1250 [==============================] - 1s 910us/step - loss: 1.2432 - accuracy: 0.4136\n",
      "1250/1250 [==============================] - 1s 851us/step - loss: 1.2461 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 844us/step - loss: 1.2378 - accuracy: 0.4168\n",
      "1250/1250 [==============================] - 1s 840us/step - loss: 1.2383 - accuracy: 0.4138\n",
      "1250/1250 [==============================] - 1s 867us/step - loss: 1.2420 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 1.2424 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 895us/step - loss: 1.2438 - accuracy: 0.4115\n",
      "1250/1250 [==============================] - 1s 952us/step - loss: 1.2466 - accuracy: 0.4101\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2388 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 1s 991us/step - loss: 1.2464 - accuracy: 0.4106\n",
      "1250/1250 [==============================] - 1s 938us/step - loss: 1.2438 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 967us/step - loss: 1.2410 - accuracy: 0.4168\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 1.2411 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 947us/step - loss: 1.2460 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 964us/step - loss: 1.2461 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 940us/step - loss: 1.2417 - accuracy: 0.4165\n",
      "1250/1250 [==============================] - 1s 952us/step - loss: 1.2448 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 962us/step - loss: 1.2428 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 1s 998us/step - loss: 1.2393 - accuracy: 0.4166\n",
      "1250/1250 [==============================] - 1s 963us/step - loss: 1.2428 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 961us/step - loss: 1.2407 - accuracy: 0.4167\n",
      "1250/1250 [==============================] - 1s 974us/step - loss: 1.2441 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 893us/step - loss: 1.2437 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 866us/step - loss: 1.2463 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 849us/step - loss: 1.2451 - accuracy: 0.4136\n",
      "1250/1250 [==============================] - 1s 938us/step - loss: 1.2404 - accuracy: 0.4162\n",
      "1250/1250 [==============================] - 1s 998us/step - loss: 1.2426 - accuracy: 0.4145\n",
      "1250/1250 [==============================] - 1s 967us/step - loss: 1.2443 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 993us/step - loss: 1.2423 - accuracy: 0.4138\n",
      "1250/1250 [==============================] - 1s 956us/step - loss: 1.2437 - accuracy: 0.4114\n",
      "1250/1250 [==============================] - 1s 962us/step - loss: 1.2423 - accuracy: 0.4120\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2408 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 1s 991us/step - loss: 1.2499 - accuracy: 0.4087\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2434 - accuracy: 0.4156\n",
      "1250/1250 [==============================] - 1s 972us/step - loss: 1.2421 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 963us/step - loss: 1.2461 - accuracy: 0.4118\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2454 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 974us/step - loss: 1.2444 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 920us/step - loss: 1.2446 - accuracy: 0.4110\n",
      "1250/1250 [==============================] - 1s 891us/step - loss: 1.2466 - accuracy: 0.4113\n",
      "1250/1250 [==============================] - 1s 846us/step - loss: 1.2428 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 1s 893us/step - loss: 1.2469 - accuracy: 0.4109\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 1.2403 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 1s 902us/step - loss: 1.2451 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2474 - accuracy: 0.4104\n",
      "1250/1250 [==============================] - 1s 948us/step - loss: 1.2435 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 998us/step - loss: 1.2403 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 1s 953us/step - loss: 1.2466 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 949us/step - loss: 1.2423 - accuracy: 0.4125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2435 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2427 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 972us/step - loss: 1.2398 - accuracy: 0.4163\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2432 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2423 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 998us/step - loss: 1.2379 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2479 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 956us/step - loss: 1.2427 - accuracy: 0.4163\n",
      "1250/1250 [==============================] - 1s 907us/step - loss: 1.2456 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 870us/step - loss: 1.2455 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 1s 853us/step - loss: 1.2429 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 1.2458 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 1.2484 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 910us/step - loss: 1.2453 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 936us/step - loss: 1.2444 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 948us/step - loss: 1.2438 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 996us/step - loss: 1.2469 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2478 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 959us/step - loss: 1.2377 - accuracy: 0.4175\n",
      "1250/1250 [==============================] - 1s 988us/step - loss: 1.2393 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 1s 971us/step - loss: 1.2481 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 983us/step - loss: 1.2430 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 959us/step - loss: 1.2478 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 957us/step - loss: 1.2474 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2486 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 1.2459 - accuracy: 0.4118\n",
      "1250/1250 [==============================] - 1s 976us/step - loss: 1.2436 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2475 - accuracy: 0.4116\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2456 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 940us/step - loss: 1.2423 - accuracy: 0.4147\n",
      "1250/1250 [==============================] - 1s 917us/step - loss: 1.2480 - accuracy: 0.4104\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 1.2452 - accuracy: 0.4186\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 1.2450 - accuracy: 0.4114\n",
      "1250/1250 [==============================] - 1s 850us/step - loss: 1.2501 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 903us/step - loss: 1.2432 - accuracy: 0.4149\n",
      "1250/1250 [==============================] - 1s 945us/step - loss: 1.2448 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 940us/step - loss: 1.2437 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 972us/step - loss: 1.2396 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 985us/step - loss: 1.2468 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 952us/step - loss: 1.2455 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 978us/step - loss: 1.2440 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2492 - accuracy: 0.4094\n",
      "1250/1250 [==============================] - 1s 956us/step - loss: 1.2402 - accuracy: 0.4165\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2432 - accuracy: 0.4149\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2455 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 955us/step - loss: 1.2400 - accuracy: 0.4168\n",
      "1250/1250 [==============================] - 1s 971us/step - loss: 1.2466 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 964us/step - loss: 1.2456 - accuracy: 0.4126\n",
      "1250/1250 [==============================] - 1s 954us/step - loss: 1.2446 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 999us/step - loss: 1.2419 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 1.2389 - accuracy: 0.4163\n",
      "1250/1250 [==============================] - 1s 956us/step - loss: 1.2445 - accuracy: 0.4115\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2419 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 1s 859us/step - loss: 1.2438 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 862us/step - loss: 1.2450 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 856us/step - loss: 1.2500 - accuracy: 0.4090\n",
      "1250/1250 [==============================] - 1s 907us/step - loss: 1.2435 - accuracy: 0.4108\n",
      "1250/1250 [==============================] - 1s 969us/step - loss: 1.2463 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2445 - accuracy: 0.4120\n",
      "1250/1250 [==============================] - 1s 997us/step - loss: 1.2416 - accuracy: 0.4159\n",
      "1250/1250 [==============================] - 1s 956us/step - loss: 1.2416 - accuracy: 0.4163\n",
      "1250/1250 [==============================] - 1s 982us/step - loss: 1.2464 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 976us/step - loss: 1.2442 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 957us/step - loss: 1.2390 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 987us/step - loss: 1.2418 - accuracy: 0.4168\n",
      "1250/1250 [==============================] - 1s 989us/step - loss: 1.2445 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 950us/step - loss: 1.2461 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 989us/step - loss: 1.2438 - accuracy: 0.4108\n",
      "1250/1250 [==============================] - 1s 986us/step - loss: 1.2351 - accuracy: 0.4178\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2427 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 1s 972us/step - loss: 1.2447 - accuracy: 0.4111\n",
      "1250/1250 [==============================] - 1s 950us/step - loss: 1.2400 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 920us/step - loss: 1.2460 - accuracy: 0.4106\n",
      "1250/1250 [==============================] - 1s 864us/step - loss: 1.2421 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 897us/step - loss: 1.2490 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 911us/step - loss: 1.2457 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 950us/step - loss: 1.2412 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 1s 998us/step - loss: 1.2433 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 967us/step - loss: 1.2447 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 972us/step - loss: 1.2444 - accuracy: 0.4110\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2424 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2462 - accuracy: 0.4108\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2475 - accuracy: 0.4078\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2414 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2374 - accuracy: 0.4168\n",
      "1250/1250 [==============================] - 1s 955us/step - loss: 1.2395 - accuracy: 0.4166\n",
      "1250/1250 [==============================] - 1s 954us/step - loss: 1.2438 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 946us/step - loss: 1.2474 - accuracy: 0.4115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 928us/step - loss: 1.2452 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 917us/step - loss: 1.2413 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 1.2451 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 888us/step - loss: 1.2439 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 941us/step - loss: 1.2433 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2479 - accuracy: 0.4105\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 1.2384 - accuracy: 0.4172\n",
      "1250/1250 [==============================] - 1s 959us/step - loss: 1.2398 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2470 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 966us/step - loss: 1.2438 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 964us/step - loss: 1.2433 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 1s 975us/step - loss: 1.2427 - accuracy: 0.4131\n",
      "1250/1250 [==============================] - 1s 995us/step - loss: 1.2450 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 961us/step - loss: 1.2452 - accuracy: 0.4126\n",
      "1250/1250 [==============================] - 1s 948us/step - loss: 1.2435 - accuracy: 0.4126\n",
      "1250/1250 [==============================] - 1s 999us/step - loss: 1.2464 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 967us/step - loss: 1.2406 - accuracy: 0.4167\n",
      "1250/1250 [==============================] - 1s 929us/step - loss: 1.2420 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 899us/step - loss: 1.2422 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 861us/step - loss: 1.2433 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 909us/step - loss: 1.2436 - accuracy: 0.4154\n",
      "1250/1250 [==============================] - 1s 945us/step - loss: 1.2359 - accuracy: 0.4206\n",
      "1250/1250 [==============================] - 1s 978us/step - loss: 1.2500 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 997us/step - loss: 1.2460 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 969us/step - loss: 1.2395 - accuracy: 0.4167\n",
      "1250/1250 [==============================] - 1s 966us/step - loss: 1.2431 - accuracy: 0.4175\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2455 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 977us/step - loss: 1.2412 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 952us/step - loss: 1.2447 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 981us/step - loss: 1.2389 - accuracy: 0.4180\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2456 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 950us/step - loss: 1.2504 - accuracy: 0.4084\n",
      "1250/1250 [==============================] - 1s 977us/step - loss: 1.2477 - accuracy: 0.4076\n",
      "1250/1250 [==============================] - 1s 998us/step - loss: 1.2469 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 942us/step - loss: 1.2438 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 893us/step - loss: 1.2461 - accuracy: 0.4115\n",
      "1250/1250 [==============================] - 1s 864us/step - loss: 1.2483 - accuracy: 0.4104\n",
      "1250/1250 [==============================] - 1s 890us/step - loss: 1.2435 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 946us/step - loss: 1.2430 - accuracy: 0.4105\n",
      "1250/1250 [==============================] - 1s 944us/step - loss: 1.2476 - accuracy: 0.4101\n",
      "1250/1250 [==============================] - 1s 966us/step - loss: 1.2424 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 994us/step - loss: 1.2416 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 1s 968us/step - loss: 1.2434 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2445 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2443 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2464 - accuracy: 0.4111\n",
      "1250/1250 [==============================] - 1s 957us/step - loss: 1.2397 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 1s 961us/step - loss: 1.2434 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 994us/step - loss: 1.2465 - accuracy: 0.4096\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 1.2366 - accuracy: 0.4169\n",
      "1250/1250 [==============================] - 1s 982us/step - loss: 1.2445 - accuracy: 0.4138\n",
      "1250/1250 [==============================] - 1s 971us/step - loss: 1.2472 - accuracy: 0.4114\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2422 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 985us/step - loss: 1.2380 - accuracy: 0.4158\n",
      "1250/1250 [==============================] - 1s 941us/step - loss: 1.2451 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 1s 896us/step - loss: 1.2430 - accuracy: 0.4120\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 1.2435 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 1.2385 - accuracy: 0.4169\n",
      "1250/1250 [==============================] - 1s 884us/step - loss: 1.2454 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 974us/step - loss: 1.2402 - accuracy: 0.4159\n",
      "1250/1250 [==============================] - 1s 987us/step - loss: 1.2408 - accuracy: 0.4166\n",
      "1250/1250 [==============================] - 1s 987us/step - loss: 1.2434 - accuracy: 0.4147\n",
      "1250/1250 [==============================] - 1s 953us/step - loss: 1.2434 - accuracy: 0.4145\n",
      "1250/1250 [==============================] - 1s 972us/step - loss: 1.2432 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2421 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2389 - accuracy: 0.4156\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2426 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 981us/step - loss: 1.2428 - accuracy: 0.4159\n",
      "1250/1250 [==============================] - 1s 995us/step - loss: 1.2445 - accuracy: 0.4126\n",
      "1250/1250 [==============================] - 1s 979us/step - loss: 1.2440 - accuracy: 0.4126\n",
      "1250/1250 [==============================] - 1s 962us/step - loss: 1.2439 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2454 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 999us/step - loss: 1.2409 - accuracy: 0.4169\n",
      "1250/1250 [==============================] - 1s 942us/step - loss: 1.2435 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 893us/step - loss: 1.2465 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 884us/step - loss: 1.2448 - accuracy: 0.4107\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 1.2475 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2414 - accuracy: 0.4169\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2439 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 979us/step - loss: 1.2454 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 971us/step - loss: 1.2489 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 968us/step - loss: 1.2397 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 987us/step - loss: 1.2472 - accuracy: 0.4122\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2422 - accuracy: 0.4120\n",
      "1250/1250 [==============================] - 1s 982us/step - loss: 1.2401 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 973us/step - loss: 1.2423 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2416 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 994us/step - loss: 1.2447 - accuracy: 0.4123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 988us/step - loss: 1.2411 - accuracy: 0.4136\n",
      "1250/1250 [==============================] - 1s 900us/step - loss: 1.2434 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 1.2456 - accuracy: 0.4131\n",
      "1250/1250 [==============================] - 1s 891us/step - loss: 1.2412 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 940us/step - loss: 1.2450 - accuracy: 0.4125\n",
      "1250/1250 [==============================] - 1s 972us/step - loss: 1.2477 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 988us/step - loss: 1.2470 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 968us/step - loss: 1.2423 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 962us/step - loss: 1.2396 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 1s 976us/step - loss: 1.2423 - accuracy: 0.4152\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2459 - accuracy: 0.4114\n",
      "1250/1250 [==============================] - 1s 977us/step - loss: 1.2449 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 979us/step - loss: 1.2441 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2437 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2423 - accuracy: 0.4152\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2428 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 995us/step - loss: 1.2386 - accuracy: 0.4181\n",
      "1250/1250 [==============================] - 1s 968us/step - loss: 1.2432 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2464 - accuracy: 0.4105\n",
      "1250/1250 [==============================] - 1s 914us/step - loss: 1.2441 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 1.2468 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 901us/step - loss: 1.2414 - accuracy: 0.4159\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2431 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 956us/step - loss: 1.2454 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 962us/step - loss: 1.2428 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 969us/step - loss: 1.2480 - accuracy: 0.4108\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2408 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 997us/step - loss: 1.2383 - accuracy: 0.4138\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2438 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 961us/step - loss: 1.2460 - accuracy: 0.4109\n",
      "1250/1250 [==============================] - 1s 971us/step - loss: 1.2415 - accuracy: 0.4149\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2435 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 992us/step - loss: 1.2406 - accuracy: 0.4175\n",
      "1250/1250 [==============================] - 1s 967us/step - loss: 1.2446 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 1s 976us/step - loss: 1.2437 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 962us/step - loss: 1.2403 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 957us/step - loss: 1.2410 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 1s 952us/step - loss: 1.2475 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 1.2426 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 893us/step - loss: 1.2424 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 1.2452 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 959us/step - loss: 1.2417 - accuracy: 0.4168\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 1.2429 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2395 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2409 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 1s 990us/step - loss: 1.2423 - accuracy: 0.4138\n",
      "1250/1250 [==============================] - 1s 961us/step - loss: 1.2450 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2366 - accuracy: 0.4203\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 1.2438 - accuracy: 0.4122\n",
      "1250/1250 [==============================] - 1s 978us/step - loss: 1.2469 - accuracy: 0.4110\n",
      "1250/1250 [==============================] - 1s 999us/step - loss: 1.2452 - accuracy: 0.4113\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2405 - accuracy: 0.4161\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 1.2405 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 1s 954us/step - loss: 1.2418 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 1s 900us/step - loss: 1.2416 - accuracy: 0.4167\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 1.2390 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 1.2449 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 924us/step - loss: 1.2391 - accuracy: 0.4145\n",
      "1250/1250 [==============================] - 1s 983us/step - loss: 1.2459 - accuracy: 0.4107\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2483 - accuracy: 0.4091\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2453 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 964us/step - loss: 1.2443 - accuracy: 0.4152\n",
      "1250/1250 [==============================] - 1s 959us/step - loss: 1.2468 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 979us/step - loss: 1.2434 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 971us/step - loss: 1.2422 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2396 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2386 - accuracy: 0.4159\n",
      "1250/1250 [==============================] - 1s 983us/step - loss: 1.2453 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 910us/step - loss: 1.2409 - accuracy: 0.4126\n",
      "1250/1250 [==============================] - 1s 888us/step - loss: 1.2464 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 915us/step - loss: 1.2454 - accuracy: 0.4109\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2410 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 976us/step - loss: 1.2411 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2416 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2448 - accuracy: 0.4125\n",
      "1250/1250 [==============================] - 1s 985us/step - loss: 1.2450 - accuracy: 0.4120\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2458 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2407 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 963us/step - loss: 1.2481 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 903us/step - loss: 1.2458 - accuracy: 0.4120\n",
      "1250/1250 [==============================] - 1s 887us/step - loss: 1.2453 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 969us/step - loss: 1.2444 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 986us/step - loss: 1.2464 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2425 - accuracy: 0.4147\n",
      "1250/1250 [==============================] - 1s 994us/step - loss: 1.2460 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 1.2387 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 974us/step - loss: 1.2432 - accuracy: 0.4126\n",
      "1250/1250 [==============================] - 1s 978us/step - loss: 1.2385 - accuracy: 0.4166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 969us/step - loss: 1.2481 - accuracy: 0.4101\n",
      "1250/1250 [==============================] - 1s 973us/step - loss: 1.2426 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 1.2430 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 928us/step - loss: 1.2452 - accuracy: 0.4110\n",
      "1250/1250 [==============================] - 1s 897us/step - loss: 1.2449 - accuracy: 0.4102\n",
      "1250/1250 [==============================] - 1s 915us/step - loss: 1.2418 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2479 - accuracy: 0.4102\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2405 - accuracy: 0.4169\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2414 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 998us/step - loss: 1.2428 - accuracy: 0.4095\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2427 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2467 - accuracy: 0.4131\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2385 - accuracy: 0.4175\n",
      "1250/1250 [==============================] - 1s 910us/step - loss: 1.2417 - accuracy: 0.4131\n",
      "1250/1250 [==============================] - 1s 888us/step - loss: 1.2432 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 867us/step - loss: 1.2441 - accuracy: 0.4113\n",
      "1250/1250 [==============================] - 1s 884us/step - loss: 1.2398 - accuracy: 0.4165\n",
      "1250/1250 [==============================] - 1s 942us/step - loss: 1.2457 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 995us/step - loss: 1.2427 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2439 - accuracy: 0.4126\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2440 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2427 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2477 - accuracy: 0.4109\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2417 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2433 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2463 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 898us/step - loss: 1.2426 - accuracy: 0.4122\n",
      "1250/1250 [==============================] - 1s 898us/step - loss: 1.2443 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 966us/step - loss: 1.2433 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2447 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 1000us/step - loss: 1.2406 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 987us/step - loss: 1.2400 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 995us/step - loss: 1.2437 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 998us/step - loss: 1.2478 - accuracy: 0.4110\n",
      "1250/1250 [==============================] - 1s 973us/step - loss: 1.2419 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 978us/step - loss: 1.2391 - accuracy: 0.4180\n",
      "1250/1250 [==============================] - 1s 989us/step - loss: 1.2362 - accuracy: 0.4169\n",
      "1250/1250 [==============================] - 1s 998us/step - loss: 1.2453 - accuracy: 0.4120\n",
      "1250/1250 [==============================] - 1s 939us/step - loss: 1.2411 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 911us/step - loss: 1.2481 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 923us/step - loss: 1.2430 - accuracy: 0.4145\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2432 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 999us/step - loss: 1.2473 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2399 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 999us/step - loss: 1.2456 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 998us/step - loss: 1.2435 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 1s 972us/step - loss: 1.2437 - accuracy: 0.4188\n",
      "1250/1250 [==============================] - 1s 977us/step - loss: 1.2444 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 986us/step - loss: 1.2496 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 962us/step - loss: 1.2427 - accuracy: 0.4149\n",
      "1250/1250 [==============================] - 1s 892us/step - loss: 1.2445 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 898us/step - loss: 1.2473 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 974us/step - loss: 1.2421 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2406 - accuracy: 0.4154\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2489 - accuracy: 0.4077\n",
      "1250/1250 [==============================] - 1s 990us/step - loss: 1.2417 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 977us/step - loss: 1.2420 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2441 - accuracy: 0.4166\n",
      "1250/1250 [==============================] - 1s 979us/step - loss: 1.2437 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 987us/step - loss: 1.2444 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2422 - accuracy: 0.4110\n",
      "1250/1250 [==============================] - 1s 949us/step - loss: 1.2442 - accuracy: 0.4163\n",
      "1250/1250 [==============================] - 1s 916us/step - loss: 1.2406 - accuracy: 0.4180\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2399 - accuracy: 0.4173\n",
      "1250/1250 [==============================] - 1s 999us/step - loss: 1.2465 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 983us/step - loss: 1.2474 - accuracy: 0.4096\n",
      "1250/1250 [==============================] - 1s 987us/step - loss: 1.2429 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 1s 974us/step - loss: 1.2451 - accuracy: 0.4118\n",
      "1250/1250 [==============================] - 1s 995us/step - loss: 1.2413 - accuracy: 0.4166\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2430 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 982us/step - loss: 1.2417 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 937us/step - loss: 1.2476 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 893us/step - loss: 1.2431 - accuracy: 0.4113\n",
      "1250/1250 [==============================] - 1s 922us/step - loss: 1.2442 - accuracy: 0.4111\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2427 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 986us/step - loss: 1.2460 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 986us/step - loss: 1.2429 - accuracy: 0.4109\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2435 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 981us/step - loss: 1.2438 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 978us/step - loss: 1.2385 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 973us/step - loss: 1.2476 - accuracy: 0.4120\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2466 - accuracy: 0.4126\n",
      "1250/1250 [==============================] - 1s 899us/step - loss: 1.2409 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 941us/step - loss: 1.2437 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 992us/step - loss: 1.2402 - accuracy: 0.4156\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2438 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 998us/step - loss: 1.2467 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 975us/step - loss: 1.2419 - accuracy: 0.4152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 974us/step - loss: 1.2455 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 975us/step - loss: 1.2472 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 952us/step - loss: 1.2438 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 985us/step - loss: 1.2447 - accuracy: 0.4115\n",
      "1250/1250 [==============================] - 1s 983us/step - loss: 1.2429 - accuracy: 0.4168\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2428 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 1s 902us/step - loss: 1.2438 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 890us/step - loss: 1.2437 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2451 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 1.2447 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 982us/step - loss: 1.2399 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 972us/step - loss: 1.2412 - accuracy: 0.4163\n",
      "1250/1250 [==============================] - 1s 982us/step - loss: 1.2409 - accuracy: 0.4167\n",
      "1250/1250 [==============================] - 1s 981us/step - loss: 1.2420 - accuracy: 0.4136\n",
      "1250/1250 [==============================] - 1s 973us/step - loss: 1.2498 - accuracy: 0.4101\n",
      "1250/1250 [==============================] - 1s 985us/step - loss: 1.2403 - accuracy: 0.4159\n",
      "1250/1250 [==============================] - 1s 986us/step - loss: 1.2480 - accuracy: 0.4126\n",
      "1250/1250 [==============================] - 1s 907us/step - loss: 1.2466 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 918us/step - loss: 1.2393 - accuracy: 0.4173\n",
      "1250/1250 [==============================] - 1s 981us/step - loss: 1.2454 - accuracy: 0.4107\n",
      "1250/1250 [==============================] - 1s 985us/step - loss: 1.2451 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 996us/step - loss: 1.2434 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 985us/step - loss: 1.2418 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 988us/step - loss: 1.2450 - accuracy: 0.4138\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2436 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2419 - accuracy: 0.4126\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2463 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2445 - accuracy: 0.4122\n",
      "1250/1250 [==============================] - 1s 915us/step - loss: 1.2445 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 1.2391 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 1s 997us/step - loss: 1.2427 - accuracy: 0.4109\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2419 - accuracy: 0.4152\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2446 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2496 - accuracy: 0.4108\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2431 - accuracy: 0.4136\n",
      "1250/1250 [==============================] - 1s 1000us/step - loss: 1.2434 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 1s 993us/step - loss: 1.2466 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 952us/step - loss: 1.2456 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 908us/step - loss: 1.2429 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2403 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 985us/step - loss: 1.2488 - accuracy: 0.4096\n",
      "1250/1250 [==============================] - 1s 994us/step - loss: 1.2480 - accuracy: 0.4107\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2409 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2454 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2402 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2438 - accuracy: 0.4131\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2438 - accuracy: 0.4120\n",
      "1250/1250 [==============================] - 1s 969us/step - loss: 1.2477 - accuracy: 0.4115\n",
      "1250/1250 [==============================] - 1s 906us/step - loss: 1.2405 - accuracy: 0.4158\n",
      "1250/1250 [==============================] - 1s 921us/step - loss: 1.2427 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 1s 959us/step - loss: 1.2408 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 982us/step - loss: 1.2421 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2440 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 993us/step - loss: 1.2433 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2425 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 982us/step - loss: 1.2420 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 982us/step - loss: 1.2441 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 993us/step - loss: 1.2399 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 1s 996us/step - loss: 1.2394 - accuracy: 0.4172\n",
      "1250/1250 [==============================] - 1s 942us/step - loss: 1.2461 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2407 - accuracy: 0.4169\n",
      "1250/1250 [==============================] - 1s 967us/step - loss: 1.2417 - accuracy: 0.4176\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2404 - accuracy: 0.4158\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2451 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 959us/step - loss: 1.2376 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 982us/step - loss: 1.2479 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2426 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2433 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 996us/step - loss: 1.2486 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2383 - accuracy: 0.4174\n",
      "1250/1250 [==============================] - 1s 945us/step - loss: 1.2454 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 1s 917us/step - loss: 1.2456 - accuracy: 0.4113\n",
      "1250/1250 [==============================] - 1s 946us/step - loss: 1.2414 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 994us/step - loss: 1.2375 - accuracy: 0.4170\n",
      "1250/1250 [==============================] - 1s 991us/step - loss: 1.2463 - accuracy: 0.4090\n",
      "1250/1250 [==============================] - 1s 986us/step - loss: 1.2482 - accuracy: 0.4102\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2394 - accuracy: 0.4152\n",
      "1250/1250 [==============================] - 1s 989us/step - loss: 1.2435 - accuracy: 0.4156\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2404 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2415 - accuracy: 0.4118\n",
      "1250/1250 [==============================] - 1s 990us/step - loss: 1.2395 - accuracy: 0.4159\n",
      "1250/1250 [==============================] - 1s 1000us/step - loss: 1.2426 - accuracy: 0.4118\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 1.2431 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 929us/step - loss: 1.2424 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 989us/step - loss: 1.2457 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2368 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 1s 983us/step - loss: 1.2453 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 986us/step - loss: 1.2414 - accuracy: 0.4159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2457 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2438 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2489 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 988us/step - loss: 1.2483 - accuracy: 0.4103\n",
      "1250/1250 [==============================] - 1s 939us/step - loss: 1.2453 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 921us/step - loss: 1.2504 - accuracy: 0.4091\n",
      "1250/1250 [==============================] - 1s 939us/step - loss: 1.2414 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 974us/step - loss: 1.2453 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2453 - accuracy: 0.4107\n",
      "1250/1250 [==============================] - 1s 990us/step - loss: 1.2415 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2464 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2464 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2435 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 997us/step - loss: 1.2429 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 996us/step - loss: 1.2446 - accuracy: 0.4116\n",
      "1250/1250 [==============================] - 1s 975us/step - loss: 1.2430 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 916us/step - loss: 1.2475 - accuracy: 0.4083\n",
      "1250/1250 [==============================] - 1s 944us/step - loss: 1.2441 - accuracy: 0.4145\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2468 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2411 - accuracy: 0.4154\n",
      "1250/1250 [==============================] - 1s 999us/step - loss: 1.2438 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2458 - accuracy: 0.4109\n",
      "1250/1250 [==============================] - 1s 990us/step - loss: 1.2434 - accuracy: 0.4152\n",
      "1250/1250 [==============================] - 1s 994us/step - loss: 1.2448 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 994us/step - loss: 1.2468 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2406 - accuracy: 0.4162\n",
      "1250/1250 [==============================] - 1s 955us/step - loss: 1.2446 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 1.2435 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 918us/step - loss: 1.2402 - accuracy: 0.4179\n",
      "1250/1250 [==============================] - 1s 960us/step - loss: 1.2426 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 1s 992us/step - loss: 1.2458 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 1s 988us/step - loss: 1.2407 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 992us/step - loss: 1.2472 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2420 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2479 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2428 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2476 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2425 - accuracy: 0.4152\n",
      "1250/1250 [==============================] - 1s 985us/step - loss: 1.2447 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 918us/step - loss: 1.2400 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 951us/step - loss: 1.2412 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 1s 972us/step - loss: 1.2445 - accuracy: 0.4125\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2450 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2479 - accuracy: 0.4098\n",
      "1250/1250 [==============================] - 1s 986us/step - loss: 1.2401 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 998us/step - loss: 1.2451 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2464 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 1s 982us/step - loss: 1.2473 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 992us/step - loss: 1.2448 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2457 - accuracy: 0.4136\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2403 - accuracy: 0.4147\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2420 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2466 - accuracy: 0.4116\n",
      "1250/1250 [==============================] - 1s 951us/step - loss: 1.2479 - accuracy: 0.4102\n",
      "1250/1250 [==============================] - 1s 914us/step - loss: 1.2446 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 922us/step - loss: 1.2450 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 1.2470 - accuracy: 0.4098\n",
      "1250/1250 [==============================] - 1s 975us/step - loss: 1.2413 - accuracy: 0.4152\n",
      "1250/1250 [==============================] - 1s 981us/step - loss: 1.2465 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2441 - accuracy: 0.4110\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2472 - accuracy: 0.4107\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2452 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2465 - accuracy: 0.4125\n",
      "1250/1250 [==============================] - 1s 997us/step - loss: 1.2457 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2421 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 995us/step - loss: 1.2433 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2473 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 947us/step - loss: 1.2445 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 911us/step - loss: 1.2408 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 949us/step - loss: 1.2425 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 991us/step - loss: 1.2438 - accuracy: 0.4169\n",
      "1250/1250 [==============================] - 1s 993us/step - loss: 1.2454 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 988us/step - loss: 1.2464 - accuracy: 0.4109\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2429 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2441 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2454 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2443 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 989us/step - loss: 1.2426 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 988us/step - loss: 1.2450 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2473 - accuracy: 0.4120\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2414 - accuracy: 0.4156\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2434 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2404 - accuracy: 0.4145\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2412 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 921us/step - loss: 1.2451 - accuracy: 0.4107\n",
      "1250/1250 [==============================] - 1s 973us/step - loss: 1.2460 - accuracy: 0.4102\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2440 - accuracy: 0.4115\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2408 - accuracy: 0.4160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2444 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2436 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 1s 959us/step - loss: 1.2430 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2454 - accuracy: 0.4116\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2438 - accuracy: 0.4136\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2472 - accuracy: 0.4114\n",
      "1250/1250 [==============================] - 1s 989us/step - loss: 1.2446 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 937us/step - loss: 1.2418 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 925us/step - loss: 1.2427 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 1.2419 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 1.2389 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 1s 982us/step - loss: 1.2410 - accuracy: 0.4156\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2434 - accuracy: 0.4145\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2438 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2412 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2388 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2410 - accuracy: 0.4152\n",
      "1250/1250 [==============================] - 1s 990us/step - loss: 1.2489 - accuracy: 0.4103\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2436 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2422 - accuracy: 0.4159\n",
      "1250/1250 [==============================] - 1s 1000us/step - loss: 1.2512 - accuracy: 0.4109\n",
      "1250/1250 [==============================] - 1s 987us/step - loss: 1.2438 - accuracy: 0.4138\n",
      "1250/1250 [==============================] - 1s 919us/step - loss: 1.2451 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 975us/step - loss: 1.2452 - accuracy: 0.4114\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2435 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 987us/step - loss: 1.2364 - accuracy: 0.4181\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2473 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2431 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2410 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2422 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2463 - accuracy: 0.4075\n",
      "1250/1250 [==============================] - 1s 998us/step - loss: 1.2470 - accuracy: 0.4094\n",
      "1250/1250 [==============================] - 1s 993us/step - loss: 1.2456 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 997us/step - loss: 1.2495 - accuracy: 0.4085\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2380 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2464 - accuracy: 0.4136\n",
      "1250/1250 [==============================] - 1s 971us/step - loss: 1.2405 - accuracy: 0.4161\n",
      "1250/1250 [==============================] - 1s 944us/step - loss: 1.2453 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 909us/step - loss: 1.2453 - accuracy: 0.4111\n",
      "1250/1250 [==============================] - 1s 928us/step - loss: 1.2476 - accuracy: 0.4122\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2458 - accuracy: 0.4122\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2447 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 995us/step - loss: 1.2426 - accuracy: 0.4138\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2451 - accuracy: 0.4122\n",
      "1250/1250 [==============================] - 1s 988us/step - loss: 1.2392 - accuracy: 0.4171\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2404 - accuracy: 0.4136\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2418 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 974us/step - loss: 1.2459 - accuracy: 0.4106\n",
      "1250/1250 [==============================] - 1s 990us/step - loss: 1.2437 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2396 - accuracy: 0.4165\n",
      "1250/1250 [==============================] - 1s 951us/step - loss: 1.2496 - accuracy: 0.4122\n",
      "1250/1250 [==============================] - 1s 912us/step - loss: 1.2483 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 953us/step - loss: 1.2446 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 987us/step - loss: 1.2466 - accuracy: 0.4126\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2470 - accuracy: 0.4110\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2444 - accuracy: 0.4114\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2434 - accuracy: 0.4138\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2442 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2488 - accuracy: 0.4099\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2457 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2411 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2397 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2422 - accuracy: 0.4158\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2408 - accuracy: 0.4145\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2418 - accuracy: 0.4114\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2463 - accuracy: 0.4107\n",
      "1250/1250 [==============================] - 1s 923us/step - loss: 1.2447 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 956us/step - loss: 1.2475 - accuracy: 0.4108\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2409 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2442 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2499 - accuracy: 0.4096\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2422 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 976us/step - loss: 1.2441 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2438 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2434 - accuracy: 0.4149\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2460 - accuracy: 0.4114\n",
      "1250/1250 [==============================] - 1s 996us/step - loss: 1.2449 - accuracy: 0.4120\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2493 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 939us/step - loss: 1.2475 - accuracy: 0.4115\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2433 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2435 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2433 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2386 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2411 - accuracy: 0.4152\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2415 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2432 - accuracy: 0.4170\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2433 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 1s 1000us/step - loss: 1.2450 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2428 - accuracy: 0.4125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2464 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 979us/step - loss: 1.2429 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2420 - accuracy: 0.4131\n",
      "1250/1250 [==============================] - 1s 982us/step - loss: 1.2445 - accuracy: 0.4098\n",
      "1250/1250 [==============================] - 1s 999us/step - loss: 1.2465 - accuracy: 0.4126\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2477 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2473 - accuracy: 0.4112\n",
      "1250/1250 [==============================] - 1s 997us/step - loss: 1.2403 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 967us/step - loss: 1.2431 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2449 - accuracy: 0.4115\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2496 - accuracy: 0.4104\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2463 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2450 - accuracy: 0.4106\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2452 - accuracy: 0.4136\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2391 - accuracy: 0.4175\n",
      "1250/1250 [==============================] - 1s 966us/step - loss: 1.2501 - accuracy: 0.4092\n",
      "1250/1250 [==============================] - 1s 975us/step - loss: 1.2423 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 1s 947us/step - loss: 1.2425 - accuracy: 0.4154\n",
      "1250/1250 [==============================] - 1s 950us/step - loss: 1.2417 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2506 - accuracy: 0.4110\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2401 - accuracy: 0.4152\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2433 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 998us/step - loss: 1.2449 - accuracy: 0.4109\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2464 - accuracy: 0.4114\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2450 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2406 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2453 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2395 - accuracy: 0.4165\n",
      "1250/1250 [==============================] - 1s 952us/step - loss: 1.2483 - accuracy: 0.4115\n",
      "1250/1250 [==============================] - 1s 965us/step - loss: 1.2435 - accuracy: 0.4097\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2461 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2467 - accuracy: 0.4101\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2438 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 988us/step - loss: 1.2376 - accuracy: 0.4165\n",
      "1250/1250 [==============================] - 1s 966us/step - loss: 1.2428 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2408 - accuracy: 0.4158\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2465 - accuracy: 0.4105\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2426 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2458 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2459 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 995us/step - loss: 1.2421 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 956us/step - loss: 1.2402 - accuracy: 0.4159\n",
      "1250/1250 [==============================] - 1s 930us/step - loss: 1.2416 - accuracy: 0.4147\n",
      "1250/1250 [==============================] - 1s 961us/step - loss: 1.2438 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2463 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2450 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2487 - accuracy: 0.4114\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2420 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2492 - accuracy: 0.4092\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2457 - accuracy: 0.4113\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2446 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2409 - accuracy: 0.4147\n",
      "1250/1250 [==============================] - 1s 946us/step - loss: 1.2398 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 1s 964us/step - loss: 1.2424 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2423 - accuracy: 0.4147\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2418 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2437 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2449 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2460 - accuracy: 0.4122\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2409 - accuracy: 0.4166\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2424 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2408 - accuracy: 0.4153\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2450 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 950us/step - loss: 1.2471 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 983us/step - loss: 1.2428 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2439 - accuracy: 0.4152\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2464 - accuracy: 0.4098\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2424 - accuracy: 0.4149\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2446 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2454 - accuracy: 0.4158\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2411 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2392 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2403 - accuracy: 0.4173\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2419 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2411 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2438 - accuracy: 0.4136\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2395 - accuracy: 0.4138\n",
      "1250/1250 [==============================] - 1s 949us/step - loss: 1.2485 - accuracy: 0.4101\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2421 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2399 - accuracy: 0.4136\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2426 - accuracy: 0.4170\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2447 - accuracy: 0.4100\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2460 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2444 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2393 - accuracy: 0.4154\n",
      "1250/1250 [==============================] - 1s 978us/step - loss: 1.2440 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 1s 942us/step - loss: 1.2459 - accuracy: 0.4120\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2430 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2482 - accuracy: 0.4094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2454 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2437 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2438 - accuracy: 0.4110\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2448 - accuracy: 0.4104\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2446 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 993us/step - loss: 1.2382 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2445 - accuracy: 0.4157\n",
      "1250/1250 [==============================] - 1s 992us/step - loss: 1.2387 - accuracy: 0.4175\n",
      "1250/1250 [==============================] - 1s 968us/step - loss: 1.2457 - accuracy: 0.4145\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2465 - accuracy: 0.4142\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2454 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2464 - accuracy: 0.4129\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2427 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2450 - accuracy: 0.4116\n",
      "1250/1250 [==============================] - 1s 997us/step - loss: 1.2449 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 1s 973us/step - loss: 1.2465 - accuracy: 0.4110\n",
      "1250/1250 [==============================] - 1s 984us/step - loss: 1.2428 - accuracy: 0.4141\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c03f053501f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearn_reg_L2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-d67cec4612bb>\u001b[0m in \u001b[0;36mlearn_reg_L2\u001b[1;34m(ROUND, l1, l2, weight)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mITERATION\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mte_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mavg_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-e84bc6f24b43>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, t, epochs, batch_size, validation_split, verbose, shuffle, workers)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m               return_dict=True)\n\u001b[0m\u001b[0;32m   1134\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracy, accuracy_weight = learn_reg_L2(5, 128, 512, weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max weight idx : 16\n",
      "max weight : 0.006600\n",
      "max accuracy : 0.420540\n"
     ]
    }
   ],
   "source": [
    "maxidx1 = accuracy.index(max(accuracy))\n",
    "print(\"max weight idx : %d\" % maxidx1)\n",
    "print(\"max weight : %f\" % accuracy_weight[maxidx1])\n",
    "print(\"max accuracy : %f\" % accuracy[maxidx1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (201,) and (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-7a44fa5c98f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\tlsgy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2761\u001b[0m     return gca().plot(\n\u001b[0;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2763\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tlsgy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \"\"\"\n\u001b[0;32m   1645\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1646\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1647\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1648\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tlsgy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tlsgy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (201,) and (100,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(step_num*2+1)], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
