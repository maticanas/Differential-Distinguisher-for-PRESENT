{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXVzTjGSQpAx",
    "outputId": "31cbc208-81e9-4251-95e5-a53c46575581"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 80-bit Key Vectors:\n",
      "Success 0x5579c1387b228445\n",
      "Success 0xe72c46c0f5945049\n",
      "Success 0xa112ffc72f68417b\n",
      "Success 0x3333dcd3213210d2\n",
      "0x5579c1387b228445\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://github.com/inmcm/present_cipher/tree/master/python\n",
    "\"\"\"\n",
    "# coding: utf-8\n",
    "from __future__ import print_function\n",
    "\n",
    "s_box = (0xC, 0x5, 0x6, 0xB, 0x9, 0x0, 0xA, 0xD, 0x3, 0xE, 0xF, 0x8, 0x4, 0x7, 0x1, 0x2)\n",
    "\n",
    "inv_s_box = (0x5, 0xE, 0xF, 0x8, 0xC, 0x1, 0x2, 0xD, 0xB, 0x4, 0x6, 0x3, 0x0, 0x7, 0x9, 0xA)\n",
    "\n",
    "p_layer_order = [0, 16, 32, 48, 1, 17, 33, 49, 2, 18, 34, 50, 3, 19, 35, 51, 4, 20, 36, 52, 5, 21, 37, 53, 6, 22, 38,\n",
    "                 54, 7, 23, 39, 55, 8, 24, 40, 56, 9, 25, 41, 57, 10, 26, 42, 58, 11, 27, 43, 59, 12, 28, 44, 60, 13,\n",
    "                 29, 45, 61, 14, 30, 46, 62, 15, 31, 47, 63]\n",
    "\n",
    "block_size = 64\n",
    "\n",
    "ROUND_LIMIT = 32\n",
    "\n",
    "\n",
    "def round_function(state, key):\n",
    "    new_state = state ^ key\n",
    "    state_nibs = []\n",
    "    for x in range(0, block_size, 4):\n",
    "        nib = (new_state >> x) & 0xF\n",
    "        sb_nib = s_box[nib]\n",
    "        state_nibs.append(sb_nib)\n",
    "    # print(state_nibs)\n",
    "\n",
    "    state_bits = []\n",
    "    for y in state_nibs:\n",
    "        nib_bits = [1 if t == '1'else 0 for t in format(y, '04b')[::-1]]\n",
    "        state_bits += nib_bits\n",
    "    # print(state_bits)\n",
    "    # print(len(state_bits))\n",
    "\n",
    "    state_p_layer = [0 for _ in range(64)]\n",
    "    for p_index, std_bits in enumerate(state_bits):\n",
    "        state_p_layer[p_layer_order[p_index]] = std_bits\n",
    "\n",
    "    # print(len(state_p_layer), state_p_layer)\n",
    "\n",
    "    round_output = 0\n",
    "    for index, ind_bit in enumerate(state_p_layer):\n",
    "        round_output += (ind_bit << index)\n",
    "\n",
    "    # print(format(round_output, '#016X'))\n",
    "\n",
    "    # print('')\n",
    "    return round_output\n",
    "\n",
    "\n",
    "def key_function_80(key, round_count):\n",
    "    # print('Start: ', hex(key))\n",
    "    # print('')\n",
    "\n",
    "    r = [1 if t == '1'else 0 for t in format(key, '080b')[::-1]]\n",
    "\n",
    "    # print('k bits:', r)\n",
    "    # print('')\n",
    "\n",
    "    h = r[-61:] + r[:-61]\n",
    "\n",
    "    # print('s bits:', h)\n",
    "    # print('')\n",
    "\n",
    "    round_key_int = 0\n",
    "    # print('init round int:', hex(round_key_int))\n",
    "    for index, ind_bit in enumerate(h):\n",
    "        round_key_int += (ind_bit << index)\n",
    "        # print('round:',index, '-', hex(round_key_int))\n",
    "\n",
    "    # print('round_key_int', hex(round_key_int))\n",
    "    # print('')\n",
    "\n",
    "    upper_nibble = round_key_int >> 76\n",
    "\n",
    "    # print('upper_nibble:', upper_nibble)\n",
    "\n",
    "    upper_nibble = s_box[upper_nibble]\n",
    "\n",
    "    # print('upper_nibble sboxed', hex(upper_nibble))\n",
    "\n",
    "    xor_portion = ((round_key_int >> 15) & 0x1F) ^ round_count\n",
    "    # print('Count:', round_count)\n",
    "    # print('XOR Value:', xor_portion)\n",
    "\n",
    "    # print('Before:', hex(round_key_int))\n",
    "    round_key_int = (round_key_int & 0x0FFFFFFFFFFFFFF07FFF) + (upper_nibble << 76) + (xor_portion << 15)\n",
    "    # print('After: ', hex(round_key_int))\n",
    "\n",
    "    return round_key_int\n",
    "\n",
    "\n",
    "\n",
    "test_vectors_80 = {1:(0x00000000000000000000, 0x0000000000000000, 0x5579C1387B228445),\n",
    "                2:(0xFFFFFFFFFFFFFFFFFFFF, 0x0000000000000000, 0xE72C46C0F5945049),\n",
    "                3:(0x00000000000000000000, 0xFFFFFFFFFFFFFFFF, 0xA112FFC72F68417B),\n",
    "                4:(0xFFFFFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF, 0x3333DCD3213210D2)}\n",
    "\n",
    "print('Testing 80-bit Key Vectors:')\n",
    "\n",
    "\n",
    "\n",
    "for test_case in test_vectors_80:\n",
    "\n",
    "    key_schedule = []\n",
    "    current_round_key = test_vectors_80[test_case][0]\n",
    "    round_state = test_vectors_80[test_case][1]\n",
    "\n",
    "    # Key schedule\n",
    "    for rnd_cnt in range(ROUND_LIMIT):\n",
    "        # print(format(round_key, '020X'))\n",
    "        # print(format(round_key >> 16, '016X'))\n",
    "        key_schedule.append(current_round_key >> 16)\n",
    "        current_round_key = key_function_80(current_round_key, rnd_cnt + 1)\n",
    "\n",
    "    for rnd in range(ROUND_LIMIT - 1):\n",
    "        # print('Round:', rnd)\n",
    "        # print('State:', format(round_state, '016X'))\n",
    "        # print('R_Key:', format(key_schedule[rnd], '016X'))\n",
    "        round_state = round_function(round_state, key_schedule[rnd])\n",
    "\n",
    "    round_state ^= key_schedule[ROUND_LIMIT-1]\n",
    "\n",
    "    if round_state == test_vectors_80[test_case][2]:\n",
    "        print('Success', hex(round_state))\n",
    "    else:\n",
    "        print('Failure', hex(round_state))\n",
    "        \n",
    "def PRESENT(P, K, ROUND):\n",
    "    key_schedule = []\n",
    "    current_round_key = K\n",
    "    round_state = P\n",
    "    \n",
    "    if(ROUND==0):\n",
    "        return P\n",
    "\n",
    "    for rnd_cnt in range(ROUND):\n",
    "        key_schedule.append(current_round_key >> 16)\n",
    "        current_round_key = key_function_80(current_round_key, rnd_cnt + 1)\n",
    "\n",
    "    for rnd in range(ROUND - 1):\n",
    "        round_state = round_function(round_state, key_schedule[rnd])\n",
    "\n",
    "    round_state ^= key_schedule[ROUND-1]\n",
    "    \n",
    "    return round_state\n",
    "\n",
    "C = PRESENT(0x0, 0x0, ROUND=32)\n",
    "print(hex(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AwZVgDHVQpAz"
   },
   "outputs": [],
   "source": [
    "Wang_diff = [0x7000000000007000, 0x0700000000000700, 0x0070000000000070, 0x0007000000000007]\n",
    "BLOCK_SIZE = 64\n",
    "\n",
    "ROUND_global = 6\n",
    "sample_num = 10000\n",
    "test_sample_num = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xJrNRizYQpA0"
   },
   "outputs": [],
   "source": [
    "def gen(sample_num, ROUND):\n",
    "    P_set = []\n",
    "    K_set = []\n",
    "    for i in range(sample_num):\n",
    "        P_set.append(random.randrange(0,2**64))\n",
    "        #print(\"%x\" % P_set[i])\n",
    "        K_set.append(random.randrange(0,2**80))\n",
    "        #print(\"%x\" % K_set[i])\n",
    "\n",
    "    C_diff_set = []\n",
    "    C_diff_label = []\n",
    "    for i in range(sample_num):\n",
    "        P = P_set[i]\n",
    "        K = K_set[i]\n",
    "        C = PRESENT(P, K, ROUND)\n",
    "        for j in range(4):\n",
    "            Cj = PRESENT(P^Wang_diff[j], K, ROUND)\n",
    "            C_diff = C^Cj\n",
    "            #print(C_diff)\n",
    "            C_diff_set.append(C_diff)\n",
    "            temp = [0, 0, 0, 0]\n",
    "            temp[j] = 1\n",
    "            C_diff_label.append(temp)\n",
    "\n",
    "    tr_X = []\n",
    "    for C_diff in C_diff_set:\n",
    "        A = []\n",
    "        for j in range(BLOCK_SIZE):\n",
    "            A.append((C_diff>>j)&1)\n",
    "        tr_X.append(A)\n",
    "        #print(A)\n",
    "    tr_X = np.array(tr_X)\n",
    "\n",
    "    tr_X = []\n",
    "    for C_diff in C_diff_set:\n",
    "        A = []\n",
    "        for j in range(BLOCK_SIZE):\n",
    "            A.append((C_diff>>j)&1)\n",
    "        tr_X.append(A)\n",
    "        #print(A)\n",
    "    tr_X = np.array(tr_X)\n",
    "    tr_t = np.array(C_diff_label)\n",
    "\n",
    "    ind = np.arange(len(tr_X))\n",
    "    np.random.shuffle(ind)\n",
    "    tr_X = tr_X[ind]\n",
    "    tr_t = tr_t[ind]\n",
    "    \n",
    "    return tr_X, tr_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gen():\n",
    "    SAMPLE_NUM_RANGE = [10000, 50000, 100000]\n",
    "    ROUND_RANGE = [3, 4, 5, 6, 7, 8]\n",
    "    for sn in SAMPLE_NUM_RANGE:\n",
    "        for rn in ROUND_RANGE:\n",
    "            tr_X, tr_t = gen(sn, rn)\n",
    "            np.save(\"ROUND %d SAMPLE %d Dataset\" % (rn, sn), tr_X)\n",
    "            np.save(\"ROUND %d SAMPLE %d Label\" % (rn, sn), tr_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Wk0Lr4ReQpA1"
   },
   "outputs": [],
   "source": [
    "def test_sample_gen():\n",
    "    TEST_SMAPLE_NUM = 10000\n",
    "    for rn in ROUND_RANGE:\n",
    "        te_X, te_t = gen(TEST_SMAPLE_NUM, rn)\n",
    "        np.save(\"ROUND %d TEST_SAMPLE Dataset\" % (rn), te_X)\n",
    "        np.save(\"ROUND %d TEST_SAMPLE Label\" % (rn), te_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O9XPTV7FQpA1"
   },
   "outputs": [],
   "source": [
    "def load_sample(SAMPLE_NUM, ROUND_NUM):\n",
    "    tr_X = np.load(\"ROUND %d SAMPLE %d Dataset.npy\" % (ROUND_NUM, SAMPLE_NUM))\n",
    "    tr_t = np.load(\"ROUND %d SAMPLE %d Label.npy\" % (ROUND_NUM, SAMPLE_NUM))\n",
    "    return tr_X, tr_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_sample(ROUND_NUM):\n",
    "    te_X = np.load(\"ROUND %d TEST_SAMPLE Dataset.npy\" % (ROUND_NUM))\n",
    "    te_t = np.load(\"ROUND %d TEST_SAMPLE Label.npy\" % (ROUND_NUM))\n",
    "    return te_X, te_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RfujwI1AQpA1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer1=128, layer2=1028, layer3=None, reg=None, learning_rate=0.001):\n",
    "        self.layers = self._build_layers(layer1, layer2, layer3, reg)\n",
    "        self.model = tf.keras.Sequential(self.layers) \n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def _build_layers(self, layer1, layer2, layer3, reg):\n",
    "        if layer3==None:\n",
    "            layers = [\n",
    "                tf.keras.layers.Flatten(input_shape=(64,)),\n",
    "                tf.keras.layers.Dense(layer1, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer2, activation='relu', kernel_regularizer=reg),\n",
    "                #tf.keras.layers.Dense(layer3, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(4, activation='softmax')\n",
    "            ]\n",
    "        else:\n",
    "            layers = [\n",
    "                tf.keras.layers.Flatten(input_shape=(64,)),\n",
    "                tf.keras.layers.Dense(layer1, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer2, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer3, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(4, activation='softmax')\n",
    "            ]\n",
    "        return layers\n",
    "\n",
    "    #그냥 cross entropy를 그대로 정의함\n",
    "    def _my_loss(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32) #float32 => int32로 casting #None, 1\n",
    "        y_true = tf.squeeze(tf.one_hot(y_true, depth=10, dtype=tf.float32), 1) # one_hot encoding #None, 1, 10 #quezze => 1을 없애줌 => None, 10\n",
    "        y_pred = tf.nn.softmax(y_pred, 1) # 한 축에 대해 softmax를 적용해라 #1 => 열을 의미 #즉, 한 행에 있는 값을 다 더하면 1이 되도록 만들어줌\n",
    "\n",
    "        #cross entropy 그대로 적용\n",
    "        #-sum t*log y 한 후에 평균 냄\n",
    "        return -tf.reduce_mean(tf.reduce_sum(\n",
    "                tf.multiply(y_true, tf.math.log(y_pred)), 1))\n",
    "\n",
    "    def _my_accuracy(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        y_true = tf.squeeze(tf.one_hot(y_true, depth=10, dtype=tf.float32), 1)\n",
    "        #argmax를 그대로 이용\n",
    "        return tf.reduce_mean(\n",
    "            tf.cast(\n",
    "                tf.equal(tf.argmax(y_true, 1), tf.argmax(y_pred, 1)), tf.float32))\n",
    "\n",
    "    def fit(self, x, t, epochs, batch_size=None, validation_split=0.0, verbose=1, shuffle=False, workers=2):\n",
    "        self.model.fit(x, t, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=verbose, shuffle=shuffle, workers=workers)\n",
    "    \n",
    "    def evaluate(self, x=None, y=None, verbose=1):\n",
    "        return self.model.evaluate(x=x, y=y, verbose=verbose)\n",
    "    \n",
    "    def summary(self):\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhQTZ7KwQpA2",
    "outputId": "85b3d255-445f-463d-ba53-122475dfd771"
   },
   "outputs": [],
   "source": [
    "#Config\n",
    "ROUND = 6\n",
    "SAMPLE_NUM = 10000\n",
    "test_sample_num = 10000\n",
    "ITERATION = 2\n",
    "\n",
    "#Fix\n",
    "batch_size = 200\n",
    "epoch_size = 25\n",
    "validation_split = 0.3\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w53eKengQpA2"
   },
   "outputs": [],
   "source": [
    "def learn(ROUND, SAMPLE_NUM, layer1, layer2, layer3=None, reg=None, learning_rate=0.001):\n",
    "    tr_X, tr_t = load_sample(SAMPLE_NUM=SAMPLE_NUM, ROUND_NUM=ROUND)\n",
    "    te_X, te_t = load_test_sample(ROUND_NUM=ROUND)\n",
    "    accuracy = []\n",
    "    for i in range(ITERATION):\n",
    "        model = MLP(layer1, layer2, layer3, reg, learning_rate)\n",
    "        model.fit(tr_X, tr_t, epochs=epoch_size, batch_size=batch_size, validation_split = validation_split, shuffle=True, verbose=0)\n",
    "        accuracy.append(model.evaluate(te_X, te_t, verbose=0)[1])\n",
    "    avg = np.mean(np.array(accuracy))\n",
    "    #print(\"average : %f\" % avg)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_reg_L1L2(ROUND=ROUND, l1=128, l2=512, weight1=[0.0001], weight2=[0.0001]):\n",
    "    tr_X, tr_t = gen(sample_num, ROUND)\n",
    "    te_X, te_t = gen(test_sample_num, ROUND)\n",
    "\n",
    "    result = []\n",
    "    result_weight = []\n",
    "    for w1 in weight1:\n",
    "        for w2 in weight2:\n",
    "            accuracy = []\n",
    "            for i in range(ITERATION):\n",
    "                model = MLP(layer1=l1, layer2=l2, reg=tf.keras.regularizers.L1L2(w1, w2))\n",
    "                model.fit(tr_X, tr_t, epochs=epoch_size, batch_size=batch_size, validation_split = validation_split, verbose=0)\n",
    "                accuracy.append(model.evaluate(te_X, te_t)[1])\n",
    "            avg_acc = np.mean(np.array(accuracy))\n",
    "            result.append(avg_acc)\n",
    "            result_weight.append([w1, w2])\n",
    "    return result, result_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2780 - accuracy: 0.4221\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2722 - accuracy: 0.4219\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2338 - accuracy: 0.4294\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2299 - accuracy: 0.4247\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2544 - accuracy: 0.4046\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2640 - accuracy: 0.3973\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3279 - accuracy: 0.3444\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3270 - accuracy: 0.3437\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2741 - accuracy: 0.3904\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2817 - accuracy: 0.3871\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2872 - accuracy: 0.3829\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2870 - accuracy: 0.3830\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3160 - accuracy: 0.3721\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3041 - accuracy: 0.3771\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3951 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3951 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.4723 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.4734 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.4713 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.4710 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.4744 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.4744 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.4749 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.4758 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.2882 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.2876 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.2884 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 2.2863 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.2740 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.2691 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.3060 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 2.3060 - accuracy: 0.2500\n",
      "[0.42198750376701355, 0.4270249903202057, 0.4009750038385391, 0.3440000116825104, 0.3887374997138977, 0.3829500079154968, 0.3745749890804291, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
      "[[0.0001, 0.0001], [0.0001, 0.001], [0.0001, 0.01], [0.0001, 0.1], [0.001, 0.0001], [0.001, 0.001], [0.001, 0.01], [0.001, 0.1], [0.01, 0.0001], [0.01, 0.001], [0.01, 0.01], [0.01, 0.1], [0.1, 0.0001], [0.1, 0.001], [0.1, 0.01], [0.1, 0.1]]\n"
     ]
    }
   ],
   "source": [
    "weight1 = [0.0001, 0.001, 0.01, 0.1]\n",
    "weight2 = [0.0001, 0.001, 0.01, 0.1]\n",
    "accuracy, accuracy_weight = learn_reg_L1L2(ROUND=5, l1=128, l2=512, weight1=weight1, weight2=weight2)\n",
    "maxidx0 = accuracy.index(max(accuracy))\n",
    "print(accuracy)\n",
    "print(accuracy_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2454 - accuracy: 0.4236\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2349 - accuracy: 0.4252\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2307 - accuracy: 0.4230\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2366 - accuracy: 0.4224\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2365 - accuracy: 0.4165\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2424 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2333 - accuracy: 0.4206\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2386 - accuracy: 0.4132\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2447 - accuracy: 0.4118\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2398 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2319 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2286 - accuracy: 0.4259\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2316 - accuracy: 0.4185\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2291 - accuracy: 0.4174\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2406 - accuracy: 0.4035\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2388 - accuracy: 0.4134\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2442 - accuracy: 0.4090\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2448 - accuracy: 0.4094\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2520 - accuracy: 0.3972\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2500 - accuracy: 0.3995\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2436 - accuracy: 0.4068\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2352 - accuracy: 0.4164\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2379 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2605 - accuracy: 0.3941\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2447 - accuracy: 0.4118\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2444 - accuracy: 0.4069\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2586 - accuracy: 0.3984\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2524 - accuracy: 0.3995\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2718 - accuracy: 0.3872\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2706 - accuracy: 0.3865\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2436 - accuracy: 0.3990\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2461 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2446 - accuracy: 0.4011\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2505 - accuracy: 0.4003\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2718 - accuracy: 0.3887\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2724 - accuracy: 0.3883\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2773 - accuracy: 0.3849\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2734 - accuracy: 0.3860\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2780 - accuracy: 0.3848\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2745 - accuracy: 0.3857\n"
     ]
    }
   ],
   "source": [
    "weight1 = [0.0001, 0.0002, 0.0003, 0.0004]\n",
    "weight2 = [0.001, 0.002, 0.003, 0.004, 0.005]\n",
    "accuracy, accuracy_weight = learn_reg_L1L2(ROUND=5, l1=128, l2=512, weight1=weight1, weight2=weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVTYS1K6uVn2",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max weight idx : 0\n",
      "max weight : [0.000100, 0.001000]\n",
      "max accuracy : 0.424412\n"
     ]
    }
   ],
   "source": [
    "maxidx1 = accuracy.index(max(accuracy))\n",
    "print(\"max weight idx : %d\" % maxidx1)\n",
    "print(\"max weight : [%f, %f]\" % (accuracy_weight[maxidx1][0], accuracy_weight[maxidx1][1]))\n",
    "print(\"max accuracy : %f\" % accuracy[maxidx1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4971 - accuracy: 0.3885\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.5134 - accuracy: 0.3827\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4389 - accuracy: 0.3929\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4252 - accuracy: 0.3952\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3839 - accuracy: 0.4016\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3957 - accuracy: 0.3972\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3774 - accuracy: 0.4017\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3740 - accuracy: 0.4008\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3625 - accuracy: 0.4022\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3379 - accuracy: 0.4043\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3268 - accuracy: 0.4048\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3196 - accuracy: 0.4048\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3185 - accuracy: 0.4077\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3082 - accuracy: 0.4069\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3040 - accuracy: 0.4079\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3097 - accuracy: 0.4069\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3011 - accuracy: 0.4051\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2872 - accuracy: 0.4106\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2955 - accuracy: 0.4070\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2908 - accuracy: 0.4072\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4497 - accuracy: 0.3881\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4489 - accuracy: 0.3929\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3881 - accuracy: 0.3983\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4145 - accuracy: 0.3992\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3749 - accuracy: 0.3992\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3787 - accuracy: 0.4038\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3411 - accuracy: 0.4069\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3456 - accuracy: 0.4070\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3253 - accuracy: 0.4086\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3132 - accuracy: 0.4065\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3167 - accuracy: 0.4095\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3025 - accuracy: 0.4070\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3018 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3163 - accuracy: 0.4076\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2858 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3049 - accuracy: 0.4070\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2992 - accuracy: 0.4072\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2738 - accuracy: 0.4162\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2727 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2848 - accuracy: 0.4098\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4033 - accuracy: 0.3957\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4091 - accuracy: 0.3975\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3700 - accuracy: 0.4043\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3596 - accuracy: 0.4017\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3464 - accuracy: 0.4086\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3557 - accuracy: 0.4040\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.3268 - accuracy: 0.4083\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3282 - accuracy: 0.4086\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3201 - accuracy: 0.4056\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3129 - accuracy: 0.4097\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3006 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2950 - accuracy: 0.4136\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3013 - accuracy: 0.4115\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2881 - accuracy: 0.4117\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2931 - accuracy: 0.4111\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2824 - accuracy: 0.4128\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.2864 - accuracy: 0.4092\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2923 - accuracy: 0.4104\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2752 - accuracy: 0.4202\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 1.2747 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3839 - accuracy: 0.4018\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3787 - accuracy: 0.3957\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3583 - accuracy: 0.4059\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3459 - accuracy: 0.4073\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3287 - accuracy: 0.4052\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3368 - accuracy: 0.4094\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3070 - accuracy: 0.4150\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2964 - accuracy: 0.4146\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2892 - accuracy: 0.4148\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2851 - accuracy: 0.4166\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2901 - accuracy: 0.4160\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2961 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2791 - accuracy: 0.4165\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2745 - accuracy: 0.4177\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2911 - accuracy: 0.4092\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2793 - accuracy: 0.4127: 0s - loss:\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2777 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2870 - accuracy: 0.4105\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2640 - accuracy: 0.4158\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2631 - accuracy: 0.4154\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3472 - accuracy: 0.4035\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3383 - accuracy: 0.4081\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3290 - accuracy: 0.4087\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3245 - accuracy: 0.4067\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3259 - accuracy: 0.4030\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2995 - accuracy: 0.4088\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2876 - accuracy: 0.4137\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2931 - accuracy: 0.4133\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2975 - accuracy: 0.4161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2895 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2790 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2851 - accuracy: 0.4119\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2721 - accuracy: 0.4121\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2704 - accuracy: 0.4162\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2640 - accuracy: 0.4165\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2631 - accuracy: 0.4167\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2634 - accuracy: 0.4176\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2683 - accuracy: 0.4190\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2611 - accuracy: 0.4172\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2690 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3486 - accuracy: 0.4006\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3188 - accuracy: 0.4088\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3047 - accuracy: 0.4118\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2990 - accuracy: 0.4161\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2907 - accuracy: 0.4159\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2890 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2792 - accuracy: 0.4190\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2800 - accuracy: 0.4173\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2683 - accuracy: 0.4153: 0s - loss: 1.268\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2807 - accuracy: 0.4123\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2722 - accuracy: 0.4168\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2619 - accuracy: 0.4158\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.2610 - accuracy: 0.4191\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2650 - accuracy: 0.4184\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2648 - accuracy: 0.4143\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2513 - accuracy: 0.4191\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2472 - accuracy: 0.4147: 0s -\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2432 - accuracy: 0.4175\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2529 - accuracy: 0.4168\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2442 - accuracy: 0.4207\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2981 - accuracy: 0.4158\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3165 - accuracy: 0.4085\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2991 - accuracy: 0.4124\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3077 - accuracy: 0.4111\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2792 - accuracy: 0.4162\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2841 - accuracy: 0.4115\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2841 - accuracy: 0.4151\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2687 - accuracy: 0.4201\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2524 - accuracy: 0.4230\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2704 - accuracy: 0.4135\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2527 - accuracy: 0.4212\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2672 - accuracy: 0.4131\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2541 - accuracy: 0.4228\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2562 - accuracy: 0.4189\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2561 - accuracy: 0.4170\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2392 - accuracy: 0.4238\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2525 - accuracy: 0.4139\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2579 - accuracy: 0.4130\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2471 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2518 - accuracy: 0.4106\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3025 - accuracy: 0.4144\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2681 - accuracy: 0.4229\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2901 - accuracy: 0.4140\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2901 - accuracy: 0.4162\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2683 - accuracy: 0.4162\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2629 - accuracy: 0.4188\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2709 - accuracy: 0.4163\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2654 - accuracy: 0.4182\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2606 - accuracy: 0.4191\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2585 - accuracy: 0.4174\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2538 - accuracy: 0.4214\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2572 - accuracy: 0.4189\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2476 - accuracy: 0.4183\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2477 - accuracy: 0.4238\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2531 - accuracy: 0.4173\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2410 - accuracy: 0.4182\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2369 - accuracy: 0.4208\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2371 - accuracy: 0.4189\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2391 - accuracy: 0.4195\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2346 - accuracy: 0.4243\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2672 - accuracy: 0.4217\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2697 - accuracy: 0.4187\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2610 - accuracy: 0.4203\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2586 - accuracy: 0.4240\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2554 - accuracy: 0.4224\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2628 - accuracy: 0.4155\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2510 - accuracy: 0.4200\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2509 - accuracy: 0.4223\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2505 - accuracy: 0.4223\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2425 - accuracy: 0.4207\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2444 - accuracy: 0.4197\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2464 - accuracy: 0.4193\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2464 - accuracy: 0.4185\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2496 - accuracy: 0.4197\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2417 - accuracy: 0.4198\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2519 - accuracy: 0.4141\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2352 - accuracy: 0.4185\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2310 - accuracy: 0.4212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2285 - accuracy: 0.4227\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2277 - accuracy: 0.4248\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2758 - accuracy: 0.4184\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.2618 - accuracy: 0.4202\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2479 - accuracy: 0.4235\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2572 - accuracy: 0.4201\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2470 - accuracy: 0.4177\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2580 - accuracy: 0.4186\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.2571 - accuracy: 0.4186\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2507 - accuracy: 0.4197\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 1.2507 - accuracy: 0.4182\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2434 - accuracy: 0.4214\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2367 - accuracy: 0.4236\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2406 - accuracy: 0.4231\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2482 - accuracy: 0.4127\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2363 - accuracy: 0.4223\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 1.2294 - accuracy: 0.4216\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 1.2373 - accuracy: 0.4184\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 1.2342 - accuracy: 0.4221\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 1.2383 - accuracy: 0.4177\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 1.2257 - accuracy: 0.4257\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 1.2263 - accuracy: 0.4254\n"
     ]
    }
   ],
   "source": [
    "weight1 = [0.00001, 0.00002, 0.00003, 0.00004, 0.00005, 0.00006, 0.00007, 0.00008, 0.00009, 0.00010]\n",
    "weight2 = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.0010]\n",
    "accuracy, accuracy_weight = learn_reg_L1L2(ROUND=5, l1=128, l2=512, weight1=weight1, weight2=weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max weight idx : 99\n",
      "max weight : [0.000100, 0.001000]\n",
      "max accuracy : 0.425513\n"
     ]
    }
   ],
   "source": [
    "maxidx1 = accuracy.index(max(accuracy))\n",
    "print(\"max weight idx : %d\" % maxidx1)\n",
    "print(\"max weight : [%f, %f]\" % (accuracy_weight[maxidx1][0], accuracy_weight[maxidx1][1]))\n",
    "print(\"max accuracy : %f\" % accuracy[maxidx1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn_reg_L1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-cb8f5e5abf90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearn_reg_L1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'learn_reg_L1' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy, accuracy_weight = learn_reg_L1(5, 128, 512, weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxidx1 = accuracy.index(max(accuracy))\n",
    "print(\"max weight idx : %d\" % maxidx1)\n",
    "print(\"max weight : %f\" % accuracy_weight[maxidx1])\n",
    "print(\"max accuracy : %f\" % accuracy[maxidx1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(step_num*2+1)], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
