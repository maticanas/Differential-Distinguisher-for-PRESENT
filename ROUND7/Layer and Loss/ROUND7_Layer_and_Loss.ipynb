{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXVzTjGSQpAx",
    "outputId": "31cbc208-81e9-4251-95e5-a53c46575581"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://github.com/inmcm/present_cipher/tree/master/python\n",
    "\"\"\"\n",
    "# coding: utf-8\n",
    "from __future__ import print_function\n",
    "\n",
    "s_box = (0xC, 0x5, 0x6, 0xB, 0x9, 0x0, 0xA, 0xD, 0x3, 0xE, 0xF, 0x8, 0x4, 0x7, 0x1, 0x2)\n",
    "\n",
    "inv_s_box = (0x5, 0xE, 0xF, 0x8, 0xC, 0x1, 0x2, 0xD, 0xB, 0x4, 0x6, 0x3, 0x0, 0x7, 0x9, 0xA)\n",
    "\n",
    "p_layer_order = [0, 16, 32, 48, 1, 17, 33, 49, 2, 18, 34, 50, 3, 19, 35, 51, 4, 20, 36, 52, 5, 21, 37, 53, 6, 22, 38,\n",
    "                 54, 7, 23, 39, 55, 8, 24, 40, 56, 9, 25, 41, 57, 10, 26, 42, 58, 11, 27, 43, 59, 12, 28, 44, 60, 13,\n",
    "                 29, 45, 61, 14, 30, 46, 62, 15, 31, 47, 63]\n",
    "\n",
    "block_size = 64\n",
    "\n",
    "ROUND_LIMIT = 32\n",
    "\n",
    "\n",
    "def round_function(state, key):\n",
    "    new_state = state ^ key\n",
    "    state_nibs = []\n",
    "    for x in range(0, block_size, 4):\n",
    "        nib = (new_state >> x) & 0xF\n",
    "        sb_nib = s_box[nib]\n",
    "        state_nibs.append(sb_nib)\n",
    "    # print(state_nibs)\n",
    "\n",
    "    state_bits = []\n",
    "    for y in state_nibs:\n",
    "        nib_bits = [1 if t == '1'else 0 for t in format(y, '04b')[::-1]]\n",
    "        state_bits += nib_bits\n",
    "    # print(state_bits)\n",
    "    # print(len(state_bits))\n",
    "\n",
    "    state_p_layer = [0 for _ in range(64)]\n",
    "    for p_index, std_bits in enumerate(state_bits):\n",
    "        state_p_layer[p_layer_order[p_index]] = std_bits\n",
    "\n",
    "    # print(len(state_p_layer), state_p_layer)\n",
    "\n",
    "    round_output = 0\n",
    "    for index, ind_bit in enumerate(state_p_layer):\n",
    "        round_output += (ind_bit << index)\n",
    "\n",
    "    # print(format(round_output, '#016X'))\n",
    "\n",
    "    # print('')\n",
    "    return round_output\n",
    "\n",
    "\n",
    "def key_function_80(key, round_count):\n",
    "    # print('Start: ', hex(key))\n",
    "    # print('')\n",
    "\n",
    "    r = [1 if t == '1'else 0 for t in format(key, '080b')[::-1]]\n",
    "\n",
    "    # print('k bits:', r)\n",
    "    # print('')\n",
    "\n",
    "    h = r[-61:] + r[:-61]\n",
    "\n",
    "    # print('s bits:', h)\n",
    "    # print('')\n",
    "\n",
    "    round_key_int = 0\n",
    "    # print('init round int:', hex(round_key_int))\n",
    "    for index, ind_bit in enumerate(h):\n",
    "        round_key_int += (ind_bit << index)\n",
    "        # print('round:',index, '-', hex(round_key_int))\n",
    "\n",
    "    # print('round_key_int', hex(round_key_int))\n",
    "    # print('')\n",
    "\n",
    "    upper_nibble = round_key_int >> 76\n",
    "\n",
    "    # print('upper_nibble:', upper_nibble)\n",
    "\n",
    "    upper_nibble = s_box[upper_nibble]\n",
    "\n",
    "    # print('upper_nibble sboxed', hex(upper_nibble))\n",
    "\n",
    "    xor_portion = ((round_key_int >> 15) & 0x1F) ^ round_count\n",
    "    # print('Count:', round_count)\n",
    "    # print('XOR Value:', xor_portion)\n",
    "\n",
    "    # print('Before:', hex(round_key_int))\n",
    "    round_key_int = (round_key_int & 0x0FFFFFFFFFFFFFF07FFF) + (upper_nibble << 76) + (xor_portion << 15)\n",
    "    # print('After: ', hex(round_key_int))\n",
    "\n",
    "    return round_key_int\n",
    "\n",
    "\n",
    "\n",
    "test_vectors_80 = {1:(0x00000000000000000000, 0x0000000000000000, 0x5579C1387B228445),\n",
    "                2:(0xFFFFFFFFFFFFFFFFFFFF, 0x0000000000000000, 0xE72C46C0F5945049),\n",
    "                3:(0x00000000000000000000, 0xFFFFFFFFFFFFFFFF, 0xA112FFC72F68417B),\n",
    "                4:(0xFFFFFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF, 0x3333DCD3213210D2)}\n",
    "        \n",
    "def PRESENT(P, K, ROUND):\n",
    "    key_schedule = []\n",
    "    current_round_key = K\n",
    "    round_state = P\n",
    "    \n",
    "    if(ROUND==0):\n",
    "        return P\n",
    "\n",
    "    for rnd_cnt in range(ROUND):\n",
    "        key_schedule.append(current_round_key >> 16)\n",
    "        current_round_key = key_function_80(current_round_key, rnd_cnt + 1)\n",
    "\n",
    "    for rnd in range(ROUND - 1):\n",
    "        round_state = round_function(round_state, key_schedule[rnd])\n",
    "\n",
    "    round_state ^= key_schedule[ROUND-1]\n",
    "    \n",
    "    return round_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AwZVgDHVQpAz"
   },
   "outputs": [],
   "source": [
    "Wang_diff = [0x7000000000007000, 0x0700000000000700, 0x0070000000000070, 0x0007000000000007]\n",
    "BLOCK_SIZE = 64\n",
    "\n",
    "ROUND_global = 6\n",
    "sample_num = 10000\n",
    "test_sample_num = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xJrNRizYQpA0"
   },
   "outputs": [],
   "source": [
    "def gen(sample_num, ROUND):\n",
    "    P_set = []\n",
    "    K_set = []\n",
    "    for i in range(sample_num):\n",
    "        P_set.append(random.randrange(0,2**64))\n",
    "        #print(\"%x\" % P_set[i])\n",
    "        K_set.append(random.randrange(0,2**80))\n",
    "        #print(\"%x\" % K_set[i])\n",
    "\n",
    "    C_diff_set = []\n",
    "    C_diff_label = []\n",
    "    for i in range(sample_num):\n",
    "        P = P_set[i]\n",
    "        K = K_set[i]\n",
    "        C = PRESENT(P, K, ROUND)\n",
    "        for j in range(4):\n",
    "            Cj = PRESENT(P^Wang_diff[j], K, ROUND)\n",
    "            C_diff = C^Cj\n",
    "            #print(C_diff)\n",
    "            C_diff_set.append(C_diff)\n",
    "            temp = [0, 0, 0, 0]\n",
    "            temp[j] = 1\n",
    "            C_diff_label.append(temp)\n",
    "\n",
    "    tr_X = []\n",
    "    for C_diff in C_diff_set:\n",
    "        A = []\n",
    "        for j in range(BLOCK_SIZE):\n",
    "            A.append((C_diff>>j)&1)\n",
    "        tr_X.append(A)\n",
    "        #print(A)\n",
    "    tr_X = np.array(tr_X)\n",
    "\n",
    "    tr_X = []\n",
    "    for C_diff in C_diff_set:\n",
    "        A = []\n",
    "        for j in range(BLOCK_SIZE):\n",
    "            A.append((C_diff>>j)&1)\n",
    "        tr_X.append(A)\n",
    "        #print(A)\n",
    "    tr_X = np.array(tr_X)\n",
    "    tr_t = np.array(C_diff_label)\n",
    "\n",
    "    ind = np.arange(len(tr_X))\n",
    "    np.random.shuffle(ind)\n",
    "    tr_X = tr_X[ind]\n",
    "    tr_t = tr_t[ind]\n",
    "    \n",
    "    return tr_X, tr_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gen():\n",
    "    SAMPLE_NUM_RANGE = [10000, 50000, 100000]\n",
    "    ROUND_RANGE = [3, 4, 5, 6, 7, 8]\n",
    "    for sn in SAMPLE_NUM_RANGE:\n",
    "        for rn in ROUND_RANGE:\n",
    "            tr_X, tr_t = gen(sn, rn)\n",
    "            np.save(\"ROUND %d SAMPLE %d Dataset\" % (rn, sn), tr_X)\n",
    "            np.save(\"ROUND %d SAMPLE %d Label\" % (rn, sn), tr_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Wk0Lr4ReQpA1"
   },
   "outputs": [],
   "source": [
    "def test_sample_gen():\n",
    "    TEST_SMAPLE_NUM = 10000\n",
    "    for rn in ROUND_RANGE:\n",
    "        te_X, te_t = gen(TEST_SMAPLE_NUM, rn)\n",
    "        np.save(\"ROUND %d TEST_SAMPLE Dataset\" % (rn), te_X)\n",
    "        np.save(\"ROUND %d TEST_SAMPLE Label\" % (rn), te_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O9XPTV7FQpA1"
   },
   "outputs": [],
   "source": [
    "def load_sample(SAMPLE_NUM, ROUND_NUM):\n",
    "    tr_X = np.load(\"ROUND %d SAMPLE %d Dataset.npy\" % (ROUND_NUM, SAMPLE_NUM))\n",
    "    tr_t = np.load(\"ROUND %d SAMPLE %d Label.npy\" % (ROUND_NUM, SAMPLE_NUM))\n",
    "    return tr_X, tr_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_sample(ROUND_NUM):\n",
    "    te_X = np.load(\"ROUND %d TEST_SAMPLE Dataset.npy\" % (ROUND_NUM))\n",
    "    te_t = np.load(\"ROUND %d TEST_SAMPLE Label.npy\" % (ROUND_NUM))\n",
    "    return te_X, te_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RfujwI1AQpA1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer1=128, layer2=1028, layer3=None, reg=None, learning_rate=0.001, loss='categorical_crossentropy'):\n",
    "        self.layers = self._build_layers(layer1, layer2, layer3, reg)\n",
    "        self.model = tf.keras.Sequential(self.layers) \n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    def _build_layers(self, layer1, layer2, layer3, reg):\n",
    "        if layer3==None:\n",
    "            layers = [\n",
    "                tf.keras.layers.Flatten(input_shape=(64,)),\n",
    "                tf.keras.layers.Dense(layer1, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer2, activation='relu', kernel_regularizer=reg),\n",
    "                #tf.keras.layers.Dense(layer3, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(4, activation='softmax')\n",
    "            ]\n",
    "        else:\n",
    "            layers = [\n",
    "                tf.keras.layers.Flatten(input_shape=(64,)),\n",
    "                tf.keras.layers.Dense(layer1, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer2, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer3, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(4, activation='softmax')\n",
    "            ]\n",
    "        return layers\n",
    "\n",
    "    #그냥 cross entropy를 그대로 정의함\n",
    "    def _my_loss(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32) #float32 => int32로 casting #None, 1\n",
    "        y_true = tf.squeeze(tf.one_hot(y_true, depth=10, dtype=tf.float32), 1) # one_hot encoding #None, 1, 10 #quezze => 1을 없애줌 => None, 10\n",
    "        y_pred = tf.nn.softmax(y_pred, 1) # 한 축에 대해 softmax를 적용해라 #1 => 열을 의미 #즉, 한 행에 있는 값을 다 더하면 1이 되도록 만들어줌\n",
    "\n",
    "        #cross entropy 그대로 적용\n",
    "        #-sum t*log y 한 후에 평균 냄\n",
    "        return -tf.reduce_mean(tf.reduce_sum(\n",
    "                tf.multiply(y_true, tf.math.log(y_pred)), 1))\n",
    "\n",
    "    def _my_accuracy(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        y_true = tf.squeeze(tf.one_hot(y_true, depth=10, dtype=tf.float32), 1)\n",
    "        #argmax를 그대로 이용\n",
    "        return tf.reduce_mean(\n",
    "            tf.cast(\n",
    "                tf.equal(tf.argmax(y_true, 1), tf.argmax(y_pred, 1)), tf.float32))\n",
    "\n",
    "    def fit(self, x, t, epochs, batch_size=None, validation_split=0.0, verbose=1, shuffle=False, workers=2):\n",
    "        return self.model.fit(x, t, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=verbose, shuffle=shuffle, workers=workers)\n",
    "    \n",
    "    def evaluate(self, x=None, y=None, verbose=1):\n",
    "        return self.model.evaluate(x=x, y=y, verbose=verbose)\n",
    "    \n",
    "    def summary(self):\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhQTZ7KwQpA2",
    "outputId": "85b3d255-445f-463d-ba53-122475dfd771"
   },
   "outputs": [],
   "source": [
    "#Config\n",
    "ROUND = 7\n",
    "SAMPLE_NUM = 10000\n",
    "test_sample_num = 10000\n",
    "ITERATION = 2\n",
    "\n",
    "#Fix\n",
    "batch_size = 200\n",
    "epoch_size = 25\n",
    "validation_split = 0.3\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w53eKengQpA2"
   },
   "outputs": [],
   "source": [
    "def learn(ROUND, SAMPLE_NUM, layer1, layer2, layer3=None, reg=None, learning_rate=0.001, loss=None):\n",
    "    tr_X, tr_t = load_sample(SAMPLE_NUM=SAMPLE_NUM, ROUND_NUM=ROUND)\n",
    "    te_X, te_t = load_test_sample(ROUND_NUM=ROUND)\n",
    "    accuracy = []\n",
    "    for i in range(ITERATION):\n",
    "        model = MLP(layer1, layer2, layer3, reg, learning_rate, loss)\n",
    "        results = model.fit(tr_X, tr_t, epochs=epoch_size, batch_size=batch_size, validation_split = validation_split, shuffle=True, verbose=0)\n",
    "        val_acc = max(results.history['val_accuracy'])\n",
    "        accuracy.append(val_acc)\n",
    "    avg = np.mean(np.array(accuracy))\n",
    "    #print(\"average : %f\" % avg)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_cand = [64, 128, 256]\n",
    "layer2_cand = [512, 1024, 2048]\n",
    "layer3_cand = [None, 1024, 2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2fQQ7n_Gba8p",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 / 512 / None \t : 0.255625\n",
      "64 / 512 / None \t : 0.255875\n",
      "64 / 512 / None \t : 0.000250\n",
      "\n",
      "\n",
      "128 / 512 / None \t : 0.258958\n",
      "128 / 512 / None \t : 0.257792\n",
      "128 / 512 / None \t : -0.001167\n",
      "\n",
      "\n",
      "256 / 512 / None \t : 0.261125\n",
      "256 / 512 / None \t : 0.262458\n",
      "256 / 512 / None \t : 0.001333\n",
      "\n",
      "\n",
      "64 / 1024 / None \t : 0.259917\n",
      "64 / 1024 / None \t : 0.256375\n",
      "64 / 1024 / None \t : -0.003542\n",
      "\n",
      "\n",
      "128 / 1024 / None \t : 0.260750\n",
      "128 / 1024 / None \t : 0.259958\n",
      "128 / 1024 / None \t : -0.000792\n",
      "\n",
      "\n",
      "256 / 1024 / None \t : 0.259583\n",
      "256 / 1024 / None \t : 0.260083\n",
      "256 / 1024 / None \t : 0.000500\n",
      "\n",
      "\n",
      "64 / 2048 / None \t : 0.258625\n",
      "64 / 2048 / None \t : 0.256667\n",
      "64 / 2048 / None \t : -0.001958\n",
      "\n",
      "\n",
      "128 / 2048 / None \t : 0.256250\n",
      "128 / 2048 / None \t : 0.257417\n",
      "128 / 2048 / None \t : 0.001167\n",
      "\n",
      "\n",
      "256 / 2048 / None \t : 0.264750\n",
      "256 / 2048 / None \t : 0.258167\n",
      "256 / 2048 / None \t : -0.006583\n",
      "\n",
      "\n",
      "64 / 512 / 1024 \t : 0.255292\n",
      "64 / 512 / 1024 \t : 0.256375\n",
      "64 / 512 / 1024 \t : 0.001083\n",
      "\n",
      "\n",
      "128 / 512 / 1024 \t : 0.257042\n",
      "128 / 512 / 1024 \t : 0.257917\n",
      "128 / 512 / 1024 \t : 0.000875\n",
      "\n",
      "\n",
      "256 / 512 / 1024 \t : 0.258000\n",
      "256 / 512 / 1024 \t : 0.257458\n",
      "256 / 512 / 1024 \t : -0.000542\n",
      "\n",
      "\n",
      "64 / 1024 / 1024 \t : 0.259167\n",
      "64 / 1024 / 1024 \t : 0.258708\n",
      "64 / 1024 / 1024 \t : -0.000458\n",
      "\n",
      "\n",
      "128 / 1024 / 1024 \t : 0.258792\n",
      "128 / 1024 / 1024 \t : 0.258333\n",
      "128 / 1024 / 1024 \t : -0.000458\n",
      "\n",
      "\n",
      "256 / 1024 / 1024 \t : 0.259833\n",
      "256 / 1024 / 1024 \t : 0.262083\n",
      "256 / 1024 / 1024 \t : 0.002250\n",
      "\n",
      "\n",
      "64 / 2048 / 1024 \t : 0.258292\n",
      "64 / 2048 / 1024 \t : 0.260083\n",
      "64 / 2048 / 1024 \t : 0.001792\n",
      "\n",
      "\n",
      "128 / 2048 / 1024 \t : 0.258417\n",
      "128 / 2048 / 1024 \t : 0.257458\n",
      "128 / 2048 / 1024 \t : -0.000958\n",
      "\n",
      "\n",
      "256 / 2048 / 1024 \t : 0.262250\n",
      "256 / 2048 / 1024 \t : 0.256125\n",
      "256 / 2048 / 1024 \t : -0.006125\n",
      "\n",
      "\n",
      "64 / 512 / 2048 \t : 0.256583\n",
      "64 / 512 / 2048 \t : 0.258458\n",
      "64 / 512 / 2048 \t : 0.001875\n",
      "\n",
      "\n",
      "128 / 512 / 2048 \t : 0.258333\n",
      "128 / 512 / 2048 \t : 0.257917\n",
      "128 / 512 / 2048 \t : -0.000417\n",
      "\n",
      "\n",
      "256 / 512 / 2048 \t : 0.259417\n",
      "256 / 512 / 2048 \t : 0.261208\n",
      "256 / 512 / 2048 \t : 0.001792\n",
      "\n",
      "\n",
      "64 / 1024 / 2048 \t : 0.257708\n",
      "64 / 1024 / 2048 \t : 0.259250\n",
      "64 / 1024 / 2048 \t : 0.001542\n",
      "\n",
      "\n",
      "128 / 1024 / 2048 \t : 0.259833\n",
      "128 / 1024 / 2048 \t : 0.260167\n",
      "128 / 1024 / 2048 \t : 0.000333\n",
      "\n",
      "\n",
      "256 / 1024 / 2048 \t : 0.256375\n",
      "256 / 1024 / 2048 \t : 0.257750\n",
      "256 / 1024 / 2048 \t : 0.001375\n",
      "\n",
      "\n",
      "64 / 2048 / 2048 \t : 0.260583\n",
      "64 / 2048 / 2048 \t : 0.258833\n",
      "64 / 2048 / 2048 \t : -0.001750\n",
      "\n",
      "\n",
      "128 / 2048 / 2048 \t : 0.258417\n",
      "128 / 2048 / 2048 \t : 0.260292\n",
      "128 / 2048 / 2048 \t : 0.001875\n",
      "\n",
      "\n",
      "256 / 2048 / 2048 \t : 0.257208\n",
      "256 / 2048 / 2048 \t : 0.257833\n",
      "256 / 2048 / 2048 \t : 0.000625\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "layer_state = []\n",
    "\n",
    "for l3 in layer3_cand:\n",
    "    for l2 in layer2_cand:\n",
    "        for l1 in layer1_cand:\n",
    "            acc_mse = learn(ROUND=ROUND, SAMPLE_NUM=SAMPLE_NUM, layer1=l1, layer2=l2, layer3=l3, reg=None, loss='mse')\n",
    "            acc_cce = learn(ROUND=ROUND, SAMPLE_NUM=SAMPLE_NUM, layer1=l1, layer2=l2, layer3=l3, reg=None, loss='categorical_crossentropy')\n",
    "            #print(acc)\n",
    "            accuracy.append([acc_mse, acc_cce])\n",
    "            layer_state.append([l1, l2, l3])\n",
    "            if(l3==None):\n",
    "                print(\"%d / %d / None \\t : %f\" % (l1, l2, acc_mse))\n",
    "                print(\"%d / %d / None \\t : %f\" % (l1, l2, acc_cce))\n",
    "                print(\"%d / %d / None \\t : %f\" % (l1, l2, acc_cce-acc_mse))\n",
    "                print(\"\\n\")\n",
    "            else:\n",
    "                print(\"%d / %d / %d \\t : %f\" % (l1, l2, l3, acc_mse))\n",
    "                print(\"%d / %d / %d \\t : %f\" % (l1, l2, l3, acc_cce))\n",
    "                print(\"%d / %d / %d \\t : %f\" % (l1, l2, l3, acc_cce-acc_mse))\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(len(accuracy)):\n",
    "    acc.append(accuracy[i][0])\n",
    "    acc.append(accuracy[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============MAX===========\n",
      "256 / 2048 : 0.264750\n",
      "with mse\n"
     ]
    }
   ],
   "source": [
    "#print(max(acc))\n",
    "idx = acc.index(max(acc))\n",
    "#print(idx)\n",
    "\n",
    "print(\"============MAX===========\")\n",
    "print(\"%d / %d : %f\" %(layer_state[int(idx/2)][0], layer_state[int(idx/2)][1], max(acc)))\n",
    "if(idx%2==0):\n",
    "    print(\"with mse\")\n",
    "if(idx%2==1):\n",
    "    print(\"with categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
