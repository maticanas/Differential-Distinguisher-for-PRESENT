{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXVzTjGSQpAx",
    "outputId": "31cbc208-81e9-4251-95e5-a53c46575581"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 80-bit Key Vectors:\n",
      "Success 0x5579c1387b228445\n",
      "Success 0xe72c46c0f5945049\n",
      "Success 0xa112ffc72f68417b\n",
      "Success 0x3333dcd3213210d2\n",
      "0x5579c1387b228445\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://github.com/inmcm/present_cipher/tree/master/python\n",
    "\"\"\"\n",
    "# coding: utf-8\n",
    "from __future__ import print_function\n",
    "\n",
    "s_box = (0xC, 0x5, 0x6, 0xB, 0x9, 0x0, 0xA, 0xD, 0x3, 0xE, 0xF, 0x8, 0x4, 0x7, 0x1, 0x2)\n",
    "\n",
    "inv_s_box = (0x5, 0xE, 0xF, 0x8, 0xC, 0x1, 0x2, 0xD, 0xB, 0x4, 0x6, 0x3, 0x0, 0x7, 0x9, 0xA)\n",
    "\n",
    "p_layer_order = [0, 16, 32, 48, 1, 17, 33, 49, 2, 18, 34, 50, 3, 19, 35, 51, 4, 20, 36, 52, 5, 21, 37, 53, 6, 22, 38,\n",
    "                 54, 7, 23, 39, 55, 8, 24, 40, 56, 9, 25, 41, 57, 10, 26, 42, 58, 11, 27, 43, 59, 12, 28, 44, 60, 13,\n",
    "                 29, 45, 61, 14, 30, 46, 62, 15, 31, 47, 63]\n",
    "\n",
    "block_size = 64\n",
    "\n",
    "ROUND_LIMIT = 32\n",
    "\n",
    "\n",
    "def round_function(state, key):\n",
    "    new_state = state ^ key\n",
    "    state_nibs = []\n",
    "    for x in range(0, block_size, 4):\n",
    "        nib = (new_state >> x) & 0xF\n",
    "        sb_nib = s_box[nib]\n",
    "        state_nibs.append(sb_nib)\n",
    "    # print(state_nibs)\n",
    "\n",
    "    state_bits = []\n",
    "    for y in state_nibs:\n",
    "        nib_bits = [1 if t == '1'else 0 for t in format(y, '04b')[::-1]]\n",
    "        state_bits += nib_bits\n",
    "    # print(state_bits)\n",
    "    # print(len(state_bits))\n",
    "\n",
    "    state_p_layer = [0 for _ in range(64)]\n",
    "    for p_index, std_bits in enumerate(state_bits):\n",
    "        state_p_layer[p_layer_order[p_index]] = std_bits\n",
    "\n",
    "    # print(len(state_p_layer), state_p_layer)\n",
    "\n",
    "    round_output = 0\n",
    "    for index, ind_bit in enumerate(state_p_layer):\n",
    "        round_output += (ind_bit << index)\n",
    "\n",
    "    # print(format(round_output, '#016X'))\n",
    "\n",
    "    # print('')\n",
    "    return round_output\n",
    "\n",
    "\n",
    "def key_function_80(key, round_count):\n",
    "    # print('Start: ', hex(key))\n",
    "    # print('')\n",
    "\n",
    "    r = [1 if t == '1'else 0 for t in format(key, '080b')[::-1]]\n",
    "\n",
    "    # print('k bits:', r)\n",
    "    # print('')\n",
    "\n",
    "    h = r[-61:] + r[:-61]\n",
    "\n",
    "    # print('s bits:', h)\n",
    "    # print('')\n",
    "\n",
    "    round_key_int = 0\n",
    "    # print('init round int:', hex(round_key_int))\n",
    "    for index, ind_bit in enumerate(h):\n",
    "        round_key_int += (ind_bit << index)\n",
    "        # print('round:',index, '-', hex(round_key_int))\n",
    "\n",
    "    # print('round_key_int', hex(round_key_int))\n",
    "    # print('')\n",
    "\n",
    "    upper_nibble = round_key_int >> 76\n",
    "\n",
    "    # print('upper_nibble:', upper_nibble)\n",
    "\n",
    "    upper_nibble = s_box[upper_nibble]\n",
    "\n",
    "    # print('upper_nibble sboxed', hex(upper_nibble))\n",
    "\n",
    "    xor_portion = ((round_key_int >> 15) & 0x1F) ^ round_count\n",
    "    # print('Count:', round_count)\n",
    "    # print('XOR Value:', xor_portion)\n",
    "\n",
    "    # print('Before:', hex(round_key_int))\n",
    "    round_key_int = (round_key_int & 0x0FFFFFFFFFFFFFF07FFF) + (upper_nibble << 76) + (xor_portion << 15)\n",
    "    # print('After: ', hex(round_key_int))\n",
    "\n",
    "    return round_key_int\n",
    "\n",
    "\n",
    "\n",
    "test_vectors_80 = {1:(0x00000000000000000000, 0x0000000000000000, 0x5579C1387B228445),\n",
    "                2:(0xFFFFFFFFFFFFFFFFFFFF, 0x0000000000000000, 0xE72C46C0F5945049),\n",
    "                3:(0x00000000000000000000, 0xFFFFFFFFFFFFFFFF, 0xA112FFC72F68417B),\n",
    "                4:(0xFFFFFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF, 0x3333DCD3213210D2)}\n",
    "\n",
    "print('Testing 80-bit Key Vectors:')\n",
    "\n",
    "\n",
    "\n",
    "for test_case in test_vectors_80:\n",
    "\n",
    "    key_schedule = []\n",
    "    current_round_key = test_vectors_80[test_case][0]\n",
    "    round_state = test_vectors_80[test_case][1]\n",
    "\n",
    "    # Key schedule\n",
    "    for rnd_cnt in range(ROUND_LIMIT):\n",
    "        # print(format(round_key, '020X'))\n",
    "        # print(format(round_key >> 16, '016X'))\n",
    "        key_schedule.append(current_round_key >> 16)\n",
    "        current_round_key = key_function_80(current_round_key, rnd_cnt + 1)\n",
    "\n",
    "    for rnd in range(ROUND_LIMIT - 1):\n",
    "        # print('Round:', rnd)\n",
    "        # print('State:', format(round_state, '016X'))\n",
    "        # print('R_Key:', format(key_schedule[rnd], '016X'))\n",
    "        round_state = round_function(round_state, key_schedule[rnd])\n",
    "\n",
    "    round_state ^= key_schedule[ROUND_LIMIT-1]\n",
    "\n",
    "    if round_state == test_vectors_80[test_case][2]:\n",
    "        print('Success', hex(round_state))\n",
    "    else:\n",
    "        print('Failure', hex(round_state))\n",
    "        \n",
    "def PRESENT(P, K, ROUND):\n",
    "    key_schedule = []\n",
    "    current_round_key = K\n",
    "    round_state = P\n",
    "    \n",
    "    if(ROUND==0):\n",
    "        return P\n",
    "\n",
    "    for rnd_cnt in range(ROUND):\n",
    "        key_schedule.append(current_round_key >> 16)\n",
    "        current_round_key = key_function_80(current_round_key, rnd_cnt + 1)\n",
    "\n",
    "    for rnd in range(ROUND - 1):\n",
    "        round_state = round_function(round_state, key_schedule[rnd])\n",
    "\n",
    "    round_state ^= key_schedule[ROUND-1]\n",
    "    \n",
    "    return round_state\n",
    "\n",
    "C = PRESENT(0x0, 0x0, ROUND=32)\n",
    "print(hex(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AwZVgDHVQpAz"
   },
   "outputs": [],
   "source": [
    "Wang_diff = [0x7000000000007000, 0x0700000000000700, 0x0070000000000070, 0x0007000000000007]\n",
    "BLOCK_SIZE = 64\n",
    "\n",
    "ROUND_global = 6\n",
    "sample_num = 10000\n",
    "test_sample_num = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xJrNRizYQpA0"
   },
   "outputs": [],
   "source": [
    "def gen(sample_num, ROUND):\n",
    "    P_set = []\n",
    "    K_set = []\n",
    "    for i in range(sample_num):\n",
    "        P_set.append(random.randrange(0,2**64))\n",
    "        #print(\"%x\" % P_set[i])\n",
    "        K_set.append(random.randrange(0,2**80))\n",
    "        #print(\"%x\" % K_set[i])\n",
    "\n",
    "    C_diff_set = []\n",
    "    C_diff_label = []\n",
    "    for i in range(sample_num):\n",
    "        P = P_set[i]\n",
    "        K = K_set[i]\n",
    "        C = PRESENT(P, K, ROUND)\n",
    "        for j in range(4):\n",
    "            Cj = PRESENT(P^Wang_diff[j], K, ROUND)\n",
    "            C_diff = C^Cj\n",
    "            #print(C_diff)\n",
    "            C_diff_set.append(C_diff)\n",
    "            temp = [0, 0, 0, 0]\n",
    "            temp[j] = 1\n",
    "            C_diff_label.append(temp)\n",
    "\n",
    "    tr_X = []\n",
    "    for C_diff in C_diff_set:\n",
    "        A = []\n",
    "        for j in range(BLOCK_SIZE):\n",
    "            A.append((C_diff>>j)&1)\n",
    "        tr_X.append(A)\n",
    "        #print(A)\n",
    "    tr_X = np.array(tr_X)\n",
    "\n",
    "    tr_X = []\n",
    "    for C_diff in C_diff_set:\n",
    "        A = []\n",
    "        for j in range(BLOCK_SIZE):\n",
    "            A.append((C_diff>>j)&1)\n",
    "        tr_X.append(A)\n",
    "        #print(A)\n",
    "    tr_X = np.array(tr_X)\n",
    "    tr_t = np.array(C_diff_label)\n",
    "\n",
    "    ind = np.arange(len(tr_X))\n",
    "    np.random.shuffle(ind)\n",
    "    tr_X = tr_X[ind]\n",
    "    tr_t = tr_t[ind]\n",
    "    \n",
    "    return tr_X, tr_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gen():\n",
    "    SAMPLE_NUM_RANGE = [10000, 50000, 100000]\n",
    "    ROUND_RANGE = [3, 4, 5, 6, 7, 8]\n",
    "    for sn in SAMPLE_NUM_RANGE:\n",
    "        for rn in ROUND_RANGE:\n",
    "            tr_X, tr_t = gen(sn, rn)\n",
    "            np.save(\"ROUND %d SAMPLE %d Dataset\" % (rn, sn), tr_X)\n",
    "            np.save(\"ROUND %d SAMPLE %d Label\" % (rn, sn), tr_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Wk0Lr4ReQpA1"
   },
   "outputs": [],
   "source": [
    "def test_sample_gen():\n",
    "    TEST_SMAPLE_NUM = 10000\n",
    "    for rn in ROUND_RANGE:\n",
    "        te_X, te_t = gen(TEST_SMAPLE_NUM, rn)\n",
    "        np.save(\"ROUND %d TEST_SAMPLE Dataset\" % (rn), te_X)\n",
    "        np.save(\"ROUND %d TEST_SAMPLE Label\" % (rn), te_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O9XPTV7FQpA1"
   },
   "outputs": [],
   "source": [
    "def load_sample(SAMPLE_NUM, ROUND_NUM):\n",
    "    tr_X = np.load(\"ROUND %d SAMPLE %d Dataset.npy\" % (ROUND_NUM, SAMPLE_NUM))\n",
    "    tr_t = np.load(\"ROUND %d SAMPLE %d Label.npy\" % (ROUND_NUM, SAMPLE_NUM))\n",
    "    return tr_X, tr_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_sample(ROUND_NUM):\n",
    "    te_X = np.load(\"ROUND %d TEST_SAMPLE Dataset.npy\" % (ROUND_NUM))\n",
    "    te_t = np.load(\"ROUND %d TEST_SAMPLE Label.npy\" % (ROUND_NUM))\n",
    "    return te_X, te_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RfujwI1AQpA1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer1=128, layer2=1028, layer3=None, reg=None, learning_rate=0.001):\n",
    "        self.layers = self._build_layers(layer1, layer2, layer3, reg)\n",
    "        self.model = tf.keras.Sequential(self.layers) \n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def _build_layers(self, layer1, layer2, layer3, reg):\n",
    "        if layer3==None:\n",
    "            layers = [\n",
    "                tf.keras.layers.Flatten(input_shape=(64,)),\n",
    "                tf.keras.layers.Dense(layer1, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer2, activation='relu', kernel_regularizer=reg),\n",
    "                #tf.keras.layers.Dense(layer3, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(4, activation='softmax')\n",
    "            ]\n",
    "        else:\n",
    "            layers = [\n",
    "                tf.keras.layers.Flatten(input_shape=(64,)),\n",
    "                tf.keras.layers.Dense(layer1, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer2, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(layer3, activation='relu', kernel_regularizer=reg),\n",
    "                tf.keras.layers.Dense(4, activation='softmax')\n",
    "            ]\n",
    "        return layers\n",
    "\n",
    "    #그냥 cross entropy를 그대로 정의함\n",
    "    def _my_loss(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32) #float32 => int32로 casting #None, 1\n",
    "        y_true = tf.squeeze(tf.one_hot(y_true, depth=10, dtype=tf.float32), 1) # one_hot encoding #None, 1, 10 #quezze => 1을 없애줌 => None, 10\n",
    "        y_pred = tf.nn.softmax(y_pred, 1) # 한 축에 대해 softmax를 적용해라 #1 => 열을 의미 #즉, 한 행에 있는 값을 다 더하면 1이 되도록 만들어줌\n",
    "\n",
    "        #cross entropy 그대로 적용\n",
    "        #-sum t*log y 한 후에 평균 냄\n",
    "        return -tf.reduce_mean(tf.reduce_sum(\n",
    "                tf.multiply(y_true, tf.math.log(y_pred)), 1))\n",
    "\n",
    "    def _my_accuracy(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        y_true = tf.squeeze(tf.one_hot(y_true, depth=10, dtype=tf.float32), 1)\n",
    "        #argmax를 그대로 이용\n",
    "        return tf.reduce_mean(\n",
    "            tf.cast(\n",
    "                tf.equal(tf.argmax(y_true, 1), tf.argmax(y_pred, 1)), tf.float32))\n",
    "\n",
    "    def fit(self, x, t, epochs, batch_size=None, validation_split=0.0, verbose=1, shuffle=False, workers=2):\n",
    "        self.model.fit(x, t, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=verbose, shuffle=shuffle, workers=workers)\n",
    "    \n",
    "    def evaluate(self, x=None, y=None, verbose=1):\n",
    "        return self.model.evaluate(x=x, y=y, verbose=verbose)\n",
    "    \n",
    "    def summary(self):\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhQTZ7KwQpA2",
    "outputId": "85b3d255-445f-463d-ba53-122475dfd771"
   },
   "outputs": [],
   "source": [
    "#Config\n",
    "ROUND = 6\n",
    "SAMPLE_NUM = 10000\n",
    "test_sample_num = 10000\n",
    "ITERATION = 2\n",
    "\n",
    "#Fix\n",
    "batch_size = 200\n",
    "epoch_size = 25\n",
    "validation_split = 0.3\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w53eKengQpA2"
   },
   "outputs": [],
   "source": [
    "def learn(ROUND, SAMPLE_NUM, layer1, layer2, layer3=None, reg=None, learning_rate=0.001):\n",
    "    tr_X, tr_t = load_sample(SAMPLE_NUM=SAMPLE_NUM, ROUND_NUM=ROUND)\n",
    "    te_X, te_t = load_test_sample(ROUND_NUM=ROUND)\n",
    "    accuracy = []\n",
    "    for i in range(ITERATION):\n",
    "        model = MLP(layer1, layer2, layer3, reg, learning_rate)\n",
    "        model.fit(tr_X, tr_t, epochs=epoch_size, batch_size=batch_size, validation_split = validation_split, shuffle=True, verbose=0)\n",
    "        accuracy.append(model.evaluate(te_X, te_t, verbose=0)[1])\n",
    "    avg = np.mean(np.array(accuracy))\n",
    "    #print(\"average : %f\" % avg)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_reg_L1L2(ROUND=ROUND, l1=256, l2=2048, weight1=[0.0001], weight2=[0.0001]):\n",
    "    tr_X, tr_t = gen(sample_num, ROUND)\n",
    "    te_X, te_t = gen(test_sample_num, ROUND)\n",
    "\n",
    "    result = []\n",
    "    result_weight = []\n",
    "    for w1 in weight1:\n",
    "        for w2 in weight2:\n",
    "            accuracy = []\n",
    "            for i in range(ITERATION):\n",
    "                model = MLP(layer1=l1, layer2=l2, reg=tf.keras.regularizers.L1L2(w1, w2))\n",
    "                model.fit(tr_X, tr_t, epochs=epoch_size, batch_size=batch_size, validation_split = validation_split, verbose=0)\n",
    "                accuracy.append(model.evaluate(te_X, te_t)[1])\n",
    "            avg_acc = np.mean(np.array(accuracy))\n",
    "            result.append(avg_acc)\n",
    "            result_weight.append([w1, w2])\n",
    "    return result, result_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3946 - accuracy: 0.2575\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 1.3957 - accuracy: 0.2553\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3926 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3936 - accuracy: 0.2545\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.4498 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4497 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.4516 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4517 - accuracy: 0.2500\n",
      "[0.25638750195503235, 0.2522374987602234, 0.25, 0.25]\n",
      "[[0.0001, 0.0001], [0.0001, 0.001], [0.001, 0.0001], [0.001, 0.001]]\n"
     ]
    }
   ],
   "source": [
    "weight1 = [0.0001, 0.001]\n",
    "weight2 = [0.0001, 0.001]\n",
    "accuracy, accuracy_weight = learn_reg_L1L2(weight1=weight1, weight2=weight2)\n",
    "maxidx0 = accuracy.index(max(accuracy))\n",
    "print(accuracy)\n",
    "print(accuracy_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.3946 - accuracy: 0.2579\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3936 - accuracy: 0.2565\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3945 - accuracy: 0.2580\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3945 - accuracy: 0.2573\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3926 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3935 - accuracy: 0.2560\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3927 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3927 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3935 - accuracy: 0.2556\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3926 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3987 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3986 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3991 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3990 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3991 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3991 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3991 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.3991 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3992 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3992 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4047 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4048 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4055 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4056 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4057 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4057 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4057 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4057 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4057 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4055 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4108 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4108 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4112 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4114 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4122 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4121 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4122 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4122 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4122 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4122 - accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "weight1 = [0.0001, 0.0002, 0.0003, 0.0004]\n",
    "weight2 = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]\n",
    "accuracy, accuracy_weight = learn_reg_L1L2(weight1=weight1, weight2=weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVTYS1K6uVn2",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max weight idx : 1\n",
      "max weight : [0.000100, 0.000200]\n",
      "max accuracy : 0.257662\n"
     ]
    }
   ],
   "source": [
    "maxidx1 = accuracy.index(max(accuracy))\n",
    "print(\"max weight idx : %d\" % maxidx1)\n",
    "print(\"max weight : [%f, %f]\" % (accuracy_weight[maxidx1][0], accuracy_weight[maxidx1][1]))\n",
    "print(\"max accuracy : %f\" % accuracy[maxidx1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.1067 - accuracy: 0.2562\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 2.0761 - accuracy: 0.2573\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7110 - accuracy: 0.2593\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7357 - accuracy: 0.2571\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.5867 - accuracy: 0.2571\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.6283 - accuracy: 0.2588\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5310 - accuracy: 0.2568\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.5247 - accuracy: 0.2576\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4831 - accuracy: 0.2593\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.4891 - accuracy: 0.2555\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4535 - accuracy: 0.2584\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4406 - accuracy: 0.2598\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4381 - accuracy: 0.2546\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.4284 - accuracy: 0.2602\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4118 - accuracy: 0.2601\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4110 - accuracy: 0.2552\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.4033 - accuracy: 0.2613\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4056 - accuracy: 0.2562\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.3968 - accuracy: 0.2624\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4043 - accuracy: 0.2573\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6331 - accuracy: 0.2561\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6489 - accuracy: 0.2545\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5273 - accuracy: 0.2599\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5437 - accuracy: 0.2600\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4978 - accuracy: 0.2582\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4919 - accuracy: 0.2623\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4436 - accuracy: 0.2605\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.4494 - accuracy: 0.2592\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4228 - accuracy: 0.2579\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4279 - accuracy: 0.2571\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4103 - accuracy: 0.2616\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4160 - accuracy: 0.2573\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4020 - accuracy: 0.2615\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4161 - accuracy: 0.2590\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3952 - accuracy: 0.2584\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3996 - accuracy: 0.2598\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3948 - accuracy: 0.2621\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3962 - accuracy: 0.2616\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3924 - accuracy: 0.2587\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3944 - accuracy: 0.2616\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5327 - accuracy: 0.2518\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5155 - accuracy: 0.2574\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4701 - accuracy: 0.2569\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4917 - accuracy: 0.2575\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4410 - accuracy: 0.2574\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4497 - accuracy: 0.2585\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4137 - accuracy: 0.2596\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4145 - accuracy: 0.2598\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4086 - accuracy: 0.2585\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3986 - accuracy: 0.2627\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4017 - accuracy: 0.2608\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4018 - accuracy: 0.2602\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.3943 - accuracy: 0.2641\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.4053 - accuracy: 0.2559\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3960 - accuracy: 0.2580\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3908 - accuracy: 0.2582\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3949 - accuracy: 0.2615\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3931 - accuracy: 0.2575\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3902 - accuracy: 0.2627\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3917 - accuracy: 0.2602\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4528 - accuracy: 0.2562\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4529 - accuracy: 0.2562\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.4403 - accuracy: 0.2592\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4432 - accuracy: 0.2561\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4231 - accuracy: 0.2577\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4211 - accuracy: 0.2596\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.4093 - accuracy: 0.2615\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4138 - accuracy: 0.2547\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3988 - accuracy: 0.2585\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3961 - accuracy: 0.2594\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3948 - accuracy: 0.2632\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3990 - accuracy: 0.2607\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.3931 - accuracy: 0.2582\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.3931 - accuracy: 0.2601\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3926 - accuracy: 0.2600\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.3918 - accuracy: 0.2614\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.3910 - accuracy: 0.2601\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.3907 - accuracy: 0.2585\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3907 - accuracy: 0.2616\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3909 - accuracy: 0.2575\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4293 - accuracy: 0.2549\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.4226 - accuracy: 0.2594\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4155 - accuracy: 0.2576\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4068 - accuracy: 0.2605\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4035 - accuracy: 0.2584\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3972 - accuracy: 0.2598\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.3949 - accuracy: 0.2572\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3952 - accuracy: 0.2614\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3931 - accuracy: 0.2596\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.3955 - accuracy: 0.2602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3922 - accuracy: 0.2606\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3908 - accuracy: 0.2599\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3930 - accuracy: 0.2612\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3921 - accuracy: 0.2589\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3911 - accuracy: 0.2549\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3925 - accuracy: 0.2616\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3907 - accuracy: 0.2588\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3904 - accuracy: 0.2575\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3909 - accuracy: 0.2581\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3905 - accuracy: 0.2580\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4156 - accuracy: 0.2596\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4063 - accuracy: 0.2582\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3966 - accuracy: 0.2582\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4061 - accuracy: 0.2592\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3957 - accuracy: 0.2614\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.3967 - accuracy: 0.2598\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3987 - accuracy: 0.2578\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3940 - accuracy: 0.2632\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3926 - accuracy: 0.2596\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3938 - accuracy: 0.2572\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3913 - accuracy: 0.2562\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3924 - accuracy: 0.2618\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.3909 - accuracy: 0.2548\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.3923 - accuracy: 0.2575\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3914 - accuracy: 0.2587\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3909 - accuracy: 0.2581\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.3907 - accuracy: 0.2547\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 1.3918 - accuracy: 0.2582\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3907 - accuracy: 0.2583\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3908 - accuracy: 0.2539\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.4028 - accuracy: 0.2592\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4021 - accuracy: 0.2619\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.3974 - accuracy: 0.2607\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3960 - accuracy: 0.2625\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3944 - accuracy: 0.2597\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3948 - accuracy: 0.2605\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3934 - accuracy: 0.2587\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3948 - accuracy: 0.2609\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3926 - accuracy: 0.2561\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3933 - accuracy: 0.2573\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3920 - accuracy: 0.2589\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3917 - accuracy: 0.2599\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3917 - accuracy: 0.2544\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3916 - accuracy: 0.2537\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.3918 - accuracy: 0.2593\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3917 - accuracy: 0.2539\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3916 - accuracy: 0.2541\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3914 - accuracy: 0.2546\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3907 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3907 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3990 - accuracy: 0.2606\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3968 - accuracy: 0.2590\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3939 - accuracy: 0.2589\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3971 - accuracy: 0.2629\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3942 - accuracy: 0.2590\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3944 - accuracy: 0.2596\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.3935 - accuracy: 0.2596\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3932 - accuracy: 0.2564\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3931 - accuracy: 0.2558\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.3923 - accuracy: 0.2537\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3921 - accuracy: 0.2539\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3923 - accuracy: 0.2528\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3922 - accuracy: 0.2562\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3921 - accuracy: 0.2531\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3921 - accuracy: 0.2533\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3921 - accuracy: 0.2537\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3913 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3921 - accuracy: 0.2529\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.3919 - accuracy: 0.2557\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3922 - accuracy: 0.2542\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3946 - accuracy: 0.2598\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3943 - accuracy: 0.2605\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3928 - accuracy: 0.2542\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3936 - accuracy: 0.2561\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3936 - accuracy: 0.2539\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3929 - accuracy: 0.2539\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3929 - accuracy: 0.2549\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3928 - accuracy: 0.2611\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3920 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.3920 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3921 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3933 - accuracy: 0.2609\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3929 - accuracy: 0.2544\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3927 - accuracy: 0.2544\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3927 - accuracy: 0.2535\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.3920 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3920 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3920 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3920 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3920 - accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3949 - accuracy: 0.2604\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3953 - accuracy: 0.2584\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3953 - accuracy: 0.2589\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3943 - accuracy: 0.2578\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3934 - accuracy: 0.2533\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3926 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3935 - accuracy: 0.2539\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3942 - accuracy: 0.2579\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3927 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3934 - accuracy: 0.2547\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3927 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3935 - accuracy: 0.2559\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3927 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3937 - accuracy: 0.2593\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3926 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3927 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3927 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3926 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 1.3927 - accuracy: 0.2500\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3927 - accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "weight1 = [0.00001, 0.00002, 0.00003, 0.00004, 0.00005, 0.00006, 0.00007, 0.00008, 0.00009, 0.00010]\n",
    "weight2 = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.0010]\n",
    "accuracy, accuracy_weight = learn_reg_L1L2(weight1=weight1, weight2=weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max weight idx : 35\n",
      "max weight : [0.000040, 0.000600]\n",
      "max accuracy : 0.261950\n"
     ]
    }
   ],
   "source": [
    "maxidx1 = accuracy.index(max(accuracy))\n",
    "print(\"max weight idx : %d\" % maxidx1)\n",
    "print(\"max weight : [%f, %f]\" % (accuracy_weight[maxidx1][0], accuracy_weight[maxidx1][1]))\n",
    "print(\"max accuracy : %f\" % accuracy[maxidx1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'step_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-7a44fa5c98f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'step_num' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot([i for i in range(step_num*2+1)], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
